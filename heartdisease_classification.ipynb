{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HeartDisease</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoking</th>\n",
       "      <th>AlcoholDrinking</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>PhysicalHealth</th>\n",
       "      <th>MentalHealth</th>\n",
       "      <th>DiffWalking</th>\n",
       "      <th>Sex</th>\n",
       "      <th>AgeCategory</th>\n",
       "      <th>Race</th>\n",
       "      <th>Diabetic</th>\n",
       "      <th>PhysicalActivity</th>\n",
       "      <th>GenHealth</th>\n",
       "      <th>SleepTime</th>\n",
       "      <th>Asthma</th>\n",
       "      <th>KidneyDisease</th>\n",
       "      <th>SkinCancer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>16.60</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Female</td>\n",
       "      <td>55-59</td>\n",
       "      <td>White</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Very good</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No</td>\n",
       "      <td>20.34</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Female</td>\n",
       "      <td>80 or older</td>\n",
       "      <td>White</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Very good</td>\n",
       "      <td>7.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "      <td>26.58</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>20.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Male</td>\n",
       "      <td>65-69</td>\n",
       "      <td>White</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Fair</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No</td>\n",
       "      <td>24.21</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Female</td>\n",
       "      <td>75-79</td>\n",
       "      <td>White</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Good</td>\n",
       "      <td>6.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No</td>\n",
       "      <td>23.71</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Female</td>\n",
       "      <td>40-44</td>\n",
       "      <td>White</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Very good</td>\n",
       "      <td>8.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  HeartDisease    BMI Smoking AlcoholDrinking Stroke  PhysicalHealth  \\\n",
       "0           No  16.60     Yes              No     No             3.0   \n",
       "1           No  20.34      No              No    Yes             0.0   \n",
       "2           No  26.58     Yes              No     No            20.0   \n",
       "3           No  24.21      No              No     No             0.0   \n",
       "4           No  23.71      No              No     No            28.0   \n",
       "\n",
       "   MentalHealth DiffWalking     Sex  AgeCategory   Race Diabetic  \\\n",
       "0          30.0          No  Female        55-59  White      Yes   \n",
       "1           0.0          No  Female  80 or older  White       No   \n",
       "2          30.0          No    Male        65-69  White      Yes   \n",
       "3           0.0          No  Female        75-79  White       No   \n",
       "4           0.0         Yes  Female        40-44  White       No   \n",
       "\n",
       "  PhysicalActivity  GenHealth  SleepTime Asthma KidneyDisease SkinCancer  \n",
       "0              Yes  Very good        5.0    Yes            No        Yes  \n",
       "1              Yes  Very good        7.0     No            No         No  \n",
       "2              Yes       Fair        8.0    Yes            No         No  \n",
       "3               No       Good        6.0     No            No        Yes  \n",
       "4              Yes  Very good        8.0     No            No         No  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('D:\\OneDrive\\MLprac\\heart_disease_prediction.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "catcols = ['Smoking', 'AlcoholDrinking', 'Stroke',\n",
    "        'DiffWalking', 'Sex', 'AgeCategory',\n",
    "       'Race', 'Diabetic', 'PhysicalActivity', 'GenHealth', \n",
    "       'Asthma', 'KidneyDisease', 'SkinCancer']\n",
    "data = pd.get_dummies(data, columns = catcols)\n",
    "data['HeartDisease'] = LabelBinarizer().fit_transform(data['HeartDisease'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(data.drop('HeartDisease', axis = 1), data['HeartDisease'], test_size= 0.2, shuffle= True, stratify= data['HeartDisease'], random_state= 777)\n",
    "x_train,x_val,y_train,y_val = train_test_split(x_train, y_train, test_size= 0.2, shuffle= True, stratify= y_train, random_state= 777)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HeartDisease</th>\n",
       "      <th>BMI</th>\n",
       "      <th>PhysicalHealth</th>\n",
       "      <th>MentalHealth</th>\n",
       "      <th>SleepTime</th>\n",
       "      <th>Smoking_No</th>\n",
       "      <th>Smoking_Yes</th>\n",
       "      <th>AlcoholDrinking_No</th>\n",
       "      <th>AlcoholDrinking_Yes</th>\n",
       "      <th>Stroke_No</th>\n",
       "      <th>...</th>\n",
       "      <th>GenHealth_Fair</th>\n",
       "      <th>GenHealth_Good</th>\n",
       "      <th>GenHealth_Poor</th>\n",
       "      <th>GenHealth_Very good</th>\n",
       "      <th>Asthma_No</th>\n",
       "      <th>Asthma_Yes</th>\n",
       "      <th>KidneyDisease_No</th>\n",
       "      <th>KidneyDisease_Yes</th>\n",
       "      <th>SkinCancer_No</th>\n",
       "      <th>SkinCancer_Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>16.60</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20.34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>26.58</td>\n",
       "      <td>20.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>24.21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>23.71</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   HeartDisease    BMI  PhysicalHealth  MentalHealth  SleepTime  Smoking_No  \\\n",
       "0             0  16.60             3.0          30.0        5.0           0   \n",
       "1             0  20.34             0.0           0.0        7.0           1   \n",
       "2             0  26.58            20.0          30.0        8.0           0   \n",
       "3             0  24.21             0.0           0.0        6.0           1   \n",
       "4             0  23.71            28.0           0.0        8.0           1   \n",
       "\n",
       "   Smoking_Yes  AlcoholDrinking_No  AlcoholDrinking_Yes  Stroke_No  ...  \\\n",
       "0            1                   1                    0          1  ...   \n",
       "1            0                   1                    0          0  ...   \n",
       "2            1                   1                    0          1  ...   \n",
       "3            0                   1                    0          1  ...   \n",
       "4            0                   1                    0          1  ...   \n",
       "\n",
       "   GenHealth_Fair  GenHealth_Good  GenHealth_Poor  GenHealth_Very good  \\\n",
       "0               0               0               0                    1   \n",
       "1               0               0               0                    1   \n",
       "2               1               0               0                    0   \n",
       "3               0               1               0                    0   \n",
       "4               0               0               0                    1   \n",
       "\n",
       "   Asthma_No  Asthma_Yes  KidneyDisease_No  KidneyDisease_Yes  SkinCancer_No  \\\n",
       "0          0           1                 1                  0              0   \n",
       "1          1           0                 1                  0              1   \n",
       "2          0           1                 1                  0              1   \n",
       "3          1           0                 1                  0              0   \n",
       "4          1           0                 1                  0              1   \n",
       "\n",
       "   SkinCancer_Yes  \n",
       "0               1  \n",
       "1               0  \n",
       "2               0  \n",
       "3               1  \n",
       "4               0  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building network\n",
    "class HeartDiseaseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        hidden1 = nn.Linear(50,300)\n",
    "        relu1 = nn.ReLU()\n",
    "        hidden2 = nn.Linear(300,300)\n",
    "        relu2 = nn.ReLU()\n",
    "        hidden3 = nn.Linear(300,1)\n",
    "        sigmoid = nn.Sigmoid()\n",
    "        self.model = nn.Sequential(hidden1,relu1,hidden2,relu2,hidden3,sigmoid)\n",
    "    def forward(self,x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing for training  \n",
    "epochs = 200\n",
    "batch_size = 1024\n",
    "batch_count = x_train.shape[0]/batch_size if x_train.shape[0]%batch_size == 0 else (x_train.shape[0]//batch_size) + 1\n",
    "learning_rate = 0.05\n",
    "loss_fn = nn.BCELoss() # loss for binary classification # Binary Cross Entropy\n",
    "model = HeartDiseaseNet()\n",
    "model = model.to(device = 'cuda')\n",
    "optim = torch.optim.SGD(model.parameters(),lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 loss: 0.6398618221282959, accuracy: 0.8134765625\n",
      "batch 20 loss: 0.27603060007095337, accuracy: 0.916015625\n",
      "batch 40 loss: 0.30925118923187256, accuracy: 0.908203125\n",
      "batch 60 loss: 0.2844991981983185, accuracy: 0.9169921875\n",
      "batch 80 loss: 0.3300231695175171, accuracy: 0.8935546875\n",
      "batch 100 loss: 0.27661675214767456, accuracy: 0.916015625\n",
      "batch 120 loss: 0.2657684087753296, accuracy: 0.91796875\n",
      "batch 140 loss: 0.30066990852355957, accuracy: 0.90625\n",
      "batch 160 loss: 0.27897727489471436, accuracy: 0.9130859375\n",
      "batch 180 loss: 0.2570582330226898, accuracy: 0.9169921875\n",
      "training loss: 0.2871100087463856, accuracy: 0.9139149865120515\n",
      "validation loss: 0.2769322097301483, accuracy: 0.9143996247654784\n",
      "batch 0 loss: 0.2934938073158264, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.2626150846481323, accuracy: 0.916015625\n",
      "batch 40 loss: 0.2849777936935425, accuracy: 0.908203125\n",
      "batch 60 loss: 0.2694835066795349, accuracy: 0.9169921875\n",
      "batch 80 loss: 0.31517842411994934, accuracy: 0.8935546875\n",
      "batch 100 loss: 0.2636227309703827, accuracy: 0.916015625\n",
      "batch 120 loss: 0.26581108570098877, accuracy: 0.91796875\n",
      "batch 140 loss: 0.29295969009399414, accuracy: 0.90625\n",
      "batch 160 loss: 0.2717415988445282, accuracy: 0.9130859375\n",
      "batch 180 loss: 0.24519658088684082, accuracy: 0.9169921875\n",
      "training loss: 0.27260134167969224, accuracy: 0.9143983849495515\n",
      "validation loss: 0.28033819794654846, accuracy: 0.9143996247654784\n",
      "batch 0 loss: 0.2988707423210144, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.2594659626483917, accuracy: 0.916015625\n",
      "batch 40 loss: 0.27575621008872986, accuracy: 0.908203125\n",
      "batch 60 loss: 0.2693713903427124, accuracy: 0.9150390625\n",
      "batch 80 loss: 0.3041512370109558, accuracy: 0.8935546875\n",
      "batch 100 loss: 0.26288533210754395, accuracy: 0.916015625\n",
      "batch 120 loss: 0.2647608518600464, accuracy: 0.91796875\n",
      "batch 140 loss: 0.2877397835254669, accuracy: 0.9052734375\n",
      "batch 160 loss: 0.26542478799819946, accuracy: 0.9130859375\n",
      "batch 180 loss: 0.23473483324050903, accuracy: 0.91796875\n",
      "training loss: 0.26749757505953314, accuracy: 0.9144269592558857\n",
      "validation loss: 0.29907432198524475, accuracy: 0.9143996247654784\n",
      "batch 0 loss: 0.31876513361930847, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.2554551362991333, accuracy: 0.916015625\n",
      "batch 40 loss: 0.2692403197288513, accuracy: 0.9072265625\n",
      "batch 60 loss: 0.2628805637359619, accuracy: 0.9150390625\n",
      "batch 80 loss: 0.2970167398452759, accuracy: 0.8935546875\n",
      "batch 100 loss: 0.2613587975502014, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.26255297660827637, accuracy: 0.91796875\n",
      "batch 140 loss: 0.28429242968559265, accuracy: 0.900390625\n",
      "batch 160 loss: 0.26169759035110474, accuracy: 0.9130859375\n",
      "batch 180 loss: 0.22863882780075073, accuracy: 0.91796875\n",
      "training loss: 0.26223302848637103, accuracy: 0.9144150259248879\n",
      "validation loss: 0.30585354566574097, accuracy: 0.9143996247654784\n",
      "batch 0 loss: 0.32662680745124817, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.24579419195652008, accuracy: 0.916015625\n",
      "batch 40 loss: 0.26573440432548523, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.2598028779029846, accuracy: 0.9140625\n",
      "batch 80 loss: 0.2924122214317322, accuracy: 0.89453125\n",
      "batch 100 loss: 0.25939100980758667, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.24924606084823608, accuracy: 0.91796875\n",
      "batch 140 loss: 0.2799606919288635, accuracy: 0.8994140625\n",
      "batch 160 loss: 0.2588462233543396, accuracy: 0.912109375\n",
      "batch 180 loss: 0.2242676317691803, accuracy: 0.91796875\n",
      "training loss: 0.2577362086623907, accuracy: 0.914594244850056\n",
      "validation loss: 0.3101356327533722, accuracy: 0.9143996247654784\n",
      "batch 0 loss: 0.3320258855819702, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.23921185731887817, accuracy: 0.9150390625\n",
      "batch 40 loss: 0.2635612189769745, accuracy: 0.912109375\n",
      "batch 60 loss: 0.2574463486671448, accuracy: 0.9130859375\n",
      "batch 80 loss: 0.2894739508628845, accuracy: 0.896484375\n",
      "batch 100 loss: 0.25799161195755005, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.2365104854106903, accuracy: 0.91796875\n",
      "batch 140 loss: 0.27645426988601685, accuracy: 0.900390625\n",
      "batch 160 loss: 0.2564394474029541, accuracy: 0.9150390625\n",
      "batch 180 loss: 0.22163920104503632, accuracy: 0.91796875\n",
      "training loss: 0.25393560610711574, accuracy: 0.914579596412556\n",
      "validation loss: 0.31021755933761597, accuracy: 0.9143996247654784\n",
      "batch 0 loss: 0.33301058411598206, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.239315927028656, accuracy: 0.916015625\n",
      "batch 40 loss: 0.2615257799625397, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.25619760155677795, accuracy: 0.912109375\n",
      "batch 80 loss: 0.2875271737575531, accuracy: 0.8984375\n",
      "batch 100 loss: 0.25802385807037354, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.23197007179260254, accuracy: 0.91796875\n",
      "batch 140 loss: 0.2776297330856323, accuracy: 0.8974609375\n",
      "batch 160 loss: 0.25446444749832153, accuracy: 0.9140625\n",
      "batch 180 loss: 0.2188645303249359, accuracy: 0.9189453125\n",
      "training loss: 0.25108302630484103, accuracy: 0.9146953387752242\n",
      "validation loss: 0.27338749170303345, accuracy: 0.914380081300813\n",
      "batch 0 loss: 0.295653760433197, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.25517845153808594, accuracy: 0.9130859375\n",
      "batch 40 loss: 0.2595217525959015, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.25549954175949097, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2860183119773865, accuracy: 0.8994140625\n",
      "batch 100 loss: 0.2543867528438568, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.22982889413833618, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.2637568712234497, accuracy: 0.904296875\n",
      "batch 160 loss: 0.2545081079006195, accuracy: 0.912109375\n",
      "batch 180 loss: 0.21664482355117798, accuracy: 0.9189453125\n",
      "training loss: 0.24848106384277344, accuracy: 0.9149939129063901\n",
      "validation loss: 0.280717134475708, accuracy: 0.914380081300813\n",
      "batch 0 loss: 0.3042107820510864, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.2516573369503021, accuracy: 0.916015625\n",
      "batch 40 loss: 0.2573377192020416, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.25690948963165283, accuracy: 0.90625\n",
      "batch 80 loss: 0.28469592332839966, accuracy: 0.8994140625\n",
      "batch 100 loss: 0.25199922919273376, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.22917307913303375, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.27375930547714233, accuracy: 0.896484375\n",
      "batch 160 loss: 0.251076340675354, accuracy: 0.912109375\n",
      "batch 180 loss: 0.2146061807870865, accuracy: 0.9189453125\n",
      "training loss: 0.2467759496718645, accuracy: 0.9148090886000559\n",
      "validation loss: 0.24811889231204987, accuracy: 0.9145755159474672\n",
      "batch 0 loss: 0.27066051959991455, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.2577698826789856, accuracy: 0.9140625\n",
      "batch 40 loss: 0.25550535321235657, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.29372847080230713, accuracy: 0.9169921875\n",
      "batch 80 loss: 0.28404751420021057, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.2495524138212204, accuracy: 0.9140625\n",
      "batch 120 loss: 0.2288803607225418, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.2664344310760498, accuracy: 0.8984375\n",
      "batch 160 loss: 0.25027868151664734, accuracy: 0.912109375\n",
      "batch 180 loss: 0.21284663677215576, accuracy: 0.9189453125\n",
      "training loss: 0.2453023450821638, accuracy: 0.9151452800938901\n",
      "validation loss: 0.24947063624858856, accuracy: 0.9145950594121326\n",
      "batch 0 loss: 0.2728378474712372, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.2579699158668518, accuracy: 0.912109375\n",
      "batch 40 loss: 0.25383973121643066, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.2726401388645172, accuracy: 0.916015625\n",
      "batch 80 loss: 0.28308582305908203, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.24855053424835205, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.22857865691184998, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.2640353739261627, accuracy: 0.8994140625\n",
      "batch 160 loss: 0.24914564192295074, accuracy: 0.912109375\n",
      "batch 180 loss: 0.21178975701332092, accuracy: 0.9189453125\n",
      "training loss: 0.24413592621684074, accuracy: 0.9152038738438901\n",
      "validation loss: 0.2593535780906677, accuracy: 0.9145364290181364\n",
      "batch 0 loss: 0.2838728427886963, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.25596189498901367, accuracy: 0.9130859375\n",
      "batch 40 loss: 0.25260722637176514, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.2714862823486328, accuracy: 0.916015625\n",
      "batch 80 loss: 0.2822335958480835, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.24663032591342926, accuracy: 0.9140625\n",
      "batch 120 loss: 0.22785398364067078, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.26294130086898804, accuracy: 0.900390625\n",
      "batch 160 loss: 0.24841943383216858, accuracy: 0.912109375\n",
      "batch 180 loss: 0.2117546647787094, accuracy: 0.919921875\n",
      "training loss: 0.24325716122984886, accuracy: 0.9152805537065584\n",
      "validation loss: 0.25948086380958557, accuracy: 0.9145559724828017\n",
      "batch 0 loss: 0.2846645712852478, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.25330308079719543, accuracy: 0.9150390625\n",
      "batch 40 loss: 0.25160571932792664, accuracy: 0.91015625\n",
      "batch 60 loss: 0.2665743827819824, accuracy: 0.916015625\n",
      "batch 80 loss: 0.28165203332901, accuracy: 0.8955078125\n",
      "batch 100 loss: 0.24453185498714447, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.2261490821838379, accuracy: 0.919921875\n",
      "batch 140 loss: 0.26128190755844116, accuracy: 0.8994140625\n",
      "batch 160 loss: 0.24783441424369812, accuracy: 0.9111328125\n",
      "batch 180 loss: 0.21098601818084717, accuracy: 0.919921875\n",
      "training loss: 0.242047051936388, accuracy: 0.9152373309627242\n",
      "validation loss: 0.25551509857177734, accuracy: 0.9145950594121326\n",
      "batch 0 loss: 0.280975341796875, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.2522275149822235, accuracy: 0.916015625\n",
      "batch 40 loss: 0.2509031295776367, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.26269006729125977, accuracy: 0.916015625\n",
      "batch 80 loss: 0.2810556888580322, accuracy: 0.89453125\n",
      "batch 100 loss: 0.24241945147514343, accuracy: 0.9140625\n",
      "batch 120 loss: 0.22603070735931396, accuracy: 0.919921875\n",
      "batch 140 loss: 0.2604730725288391, accuracy: 0.8994140625\n",
      "batch 160 loss: 0.2482738196849823, accuracy: 0.912109375\n",
      "batch 180 loss: 0.21051783859729767, accuracy: 0.9189453125\n",
      "training loss: 0.2411882010102272, accuracy: 0.9152812762752243\n",
      "validation loss: 0.2525516450405121, accuracy: 0.9145755159474672\n",
      "batch 0 loss: 0.278262197971344, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.2525804042816162, accuracy: 0.916015625\n",
      "batch 40 loss: 0.2504797577857971, accuracy: 0.91015625\n",
      "batch 60 loss: 0.25347569584846497, accuracy: 0.9169921875\n",
      "batch 80 loss: 0.2802826166152954, accuracy: 0.89453125\n",
      "batch 100 loss: 0.24203616380691528, accuracy: 0.9140625\n",
      "batch 120 loss: 0.22644875943660736, accuracy: 0.919921875\n",
      "batch 140 loss: 0.25973978638648987, accuracy: 0.8984375\n",
      "batch 160 loss: 0.24866701662540436, accuracy: 0.9111328125\n",
      "batch 180 loss: 0.20977437496185303, accuracy: 0.9189453125\n",
      "training loss: 0.24046510353684425, accuracy: 0.9153740497127242\n",
      "validation loss: 0.251720130443573, accuracy: 0.9145950594121326\n",
      "batch 0 loss: 0.2777252197265625, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.2519229054450989, accuracy: 0.9169921875\n",
      "batch 40 loss: 0.2504151463508606, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24976232647895813, accuracy: 0.9169921875\n",
      "batch 80 loss: 0.2791188359260559, accuracy: 0.896484375\n",
      "batch 100 loss: 0.24072474241256714, accuracy: 0.9140625\n",
      "batch 120 loss: 0.22543206810951233, accuracy: 0.919921875\n",
      "batch 140 loss: 0.2583552598953247, accuracy: 0.90234375\n",
      "batch 160 loss: 0.24793066084384918, accuracy: 0.912109375\n",
      "batch 180 loss: 0.2093690037727356, accuracy: 0.9189453125\n",
      "training loss: 0.23958309911191464, accuracy: 0.9154424090877242\n",
      "validation loss: 0.25087738037109375, accuracy: 0.9146732332707942\n",
      "batch 0 loss: 0.27711161971092224, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.2519699037075043, accuracy: 0.91796875\n",
      "batch 40 loss: 0.25027650594711304, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.2488885521888733, accuracy: 0.9150390625\n",
      "batch 80 loss: 0.27881282567977905, accuracy: 0.896484375\n",
      "batch 100 loss: 0.23930838704109192, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.22442679107189178, accuracy: 0.919921875\n",
      "batch 140 loss: 0.25617995858192444, accuracy: 0.90234375\n",
      "batch 160 loss: 0.24781166017055511, accuracy: 0.9111328125\n",
      "batch 180 loss: 0.2092222273349762, accuracy: 0.919921875\n",
      "training loss: 0.23888899609446526, accuracy: 0.9154661005815583\n",
      "validation loss: 0.24826662242412567, accuracy: 0.9147514071294559\n",
      "batch 0 loss: 0.2745440602302551, accuracy: 0.9140625\n",
      "batch 20 loss: 0.2526988983154297, accuracy: 0.919921875\n",
      "batch 40 loss: 0.2501264214515686, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24732853472232819, accuracy: 0.9140625\n",
      "batch 80 loss: 0.2819618582725525, accuracy: 0.888671875\n",
      "batch 100 loss: 0.23922252655029297, accuracy: 0.912109375\n",
      "batch 120 loss: 0.2228846400976181, accuracy: 0.91796875\n",
      "batch 140 loss: 0.251865029335022, accuracy: 0.9033203125\n",
      "batch 160 loss: 0.24829404056072235, accuracy: 0.9072265625\n",
      "batch 180 loss: 0.2104220688343048, accuracy: 0.9208984375\n",
      "training loss: 0.23758705340325834, accuracy: 0.9153879755815584\n",
      "validation loss: 0.24800175428390503, accuracy: 0.9147709505941213\n",
      "batch 0 loss: 0.2745766341686249, accuracy: 0.9140625\n",
      "batch 20 loss: 0.2528238594532013, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24958012998104095, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24782077968120575, accuracy: 0.9140625\n",
      "batch 80 loss: 0.28037208318710327, accuracy: 0.888671875\n",
      "batch 100 loss: 0.23347990214824677, accuracy: 0.912109375\n",
      "batch 120 loss: 0.22213421761989594, accuracy: 0.91796875\n",
      "batch 140 loss: 0.25637224316596985, accuracy: 0.896484375\n",
      "batch 160 loss: 0.24486736953258514, accuracy: 0.9130859375\n",
      "batch 180 loss: 0.20764486491680145, accuracy: 0.9189453125\n",
      "training loss: 0.2374796860665083, accuracy: 0.9154409639503923\n",
      "validation loss: 0.2500961720943451, accuracy: 0.914712320200125\n",
      "batch 0 loss: 0.27719831466674805, accuracy: 0.9140625\n",
      "batch 20 loss: 0.2513620853424072, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.2495209127664566, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.2485257089138031, accuracy: 0.9091796875\n",
      "batch 80 loss: 0.2806601822376251, accuracy: 0.8916015625\n",
      "batch 100 loss: 0.24007786810398102, accuracy: 0.9140625\n",
      "batch 120 loss: 0.22307206690311432, accuracy: 0.919921875\n",
      "batch 140 loss: 0.25349241495132446, accuracy: 0.90234375\n",
      "batch 160 loss: 0.24997973442077637, accuracy: 0.91015625\n",
      "batch 180 loss: 0.20762640237808228, accuracy: 0.9189453125\n",
      "training loss: 0.23753682315349578, accuracy: 0.9153481905128923\n",
      "validation loss: 0.246670663356781, accuracy: 0.9149272983114447\n",
      "batch 0 loss: 0.2736000716686249, accuracy: 0.9140625\n",
      "batch 20 loss: 0.2498144805431366, accuracy: 0.91796875\n",
      "batch 40 loss: 0.2495577186346054, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24887007474899292, accuracy: 0.908203125\n",
      "batch 80 loss: 0.2805512249469757, accuracy: 0.892578125\n",
      "batch 100 loss: 0.23846890032291412, accuracy: 0.9140625\n",
      "batch 120 loss: 0.22273212671279907, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.25315162539482117, accuracy: 0.90234375\n",
      "batch 160 loss: 0.2496953010559082, accuracy: 0.91015625\n",
      "batch 180 loss: 0.20757687091827393, accuracy: 0.919921875\n",
      "training loss: 0.23702576205134393, accuracy: 0.9153530733253923\n",
      "validation loss: 0.2445695847272873, accuracy: 0.9148882113821138\n",
      "batch 0 loss: 0.2714437246322632, accuracy: 0.9140625\n",
      "batch 20 loss: 0.2491905391216278, accuracy: 0.91796875\n",
      "batch 40 loss: 0.24936771392822266, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24926051497459412, accuracy: 0.908203125\n",
      "batch 80 loss: 0.28058671951293945, accuracy: 0.890625\n",
      "batch 100 loss: 0.23755308985710144, accuracy: 0.9140625\n",
      "batch 120 loss: 0.22262252867221832, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.2533003091812134, accuracy: 0.90234375\n",
      "batch 160 loss: 0.24962860345840454, accuracy: 0.91015625\n",
      "batch 180 loss: 0.20732276141643524, accuracy: 0.919921875\n",
      "training loss: 0.23659485578536987, accuracy: 0.9153830927690584\n",
      "validation loss: 0.24189460277557373, accuracy: 0.9151227329580988\n",
      "batch 0 loss: 0.2686212956905365, accuracy: 0.9140625\n",
      "batch 20 loss: 0.2486453354358673, accuracy: 0.91796875\n",
      "batch 40 loss: 0.24939826130867004, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.249676913022995, accuracy: 0.908203125\n",
      "batch 80 loss: 0.2799721360206604, accuracy: 0.8916015625\n",
      "batch 100 loss: 0.23622864484786987, accuracy: 0.9140625\n",
      "batch 120 loss: 0.222456157207489, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.2527778744697571, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.24895533919334412, accuracy: 0.9072265625\n",
      "batch 180 loss: 0.2072453647851944, accuracy: 0.921875\n",
      "training loss: 0.23629080906510352, accuracy: 0.9154277606502242\n",
      "validation loss: 0.24102920293807983, accuracy: 0.9152009068167605\n",
      "batch 0 loss: 0.26789936423301697, accuracy: 0.9140625\n",
      "batch 20 loss: 0.24782617390155792, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24922187626361847, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24857372045516968, accuracy: 0.9091796875\n",
      "batch 80 loss: 0.2796227037906647, accuracy: 0.8916015625\n",
      "batch 100 loss: 0.23535574972629547, accuracy: 0.9140625\n",
      "batch 120 loss: 0.22247646749019623, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.2539687156677246, accuracy: 0.8994140625\n",
      "batch 160 loss: 0.24891206622123718, accuracy: 0.908203125\n",
      "batch 180 loss: 0.20740416646003723, accuracy: 0.919921875\n",
      "training loss: 0.23588643543422222, accuracy: 0.9154717059627242\n",
      "validation loss: 0.23932239413261414, accuracy: 0.9152790806754222\n",
      "batch 0 loss: 0.2661440372467041, accuracy: 0.9140625\n",
      "batch 20 loss: 0.24627196788787842, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24887342751026154, accuracy: 0.912109375\n",
      "batch 60 loss: 0.24871587753295898, accuracy: 0.9091796875\n",
      "batch 80 loss: 0.2790476381778717, accuracy: 0.8916015625\n",
      "batch 100 loss: 0.23511245846748352, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.22202859818935394, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.25340497493743896, accuracy: 0.8994140625\n",
      "batch 160 loss: 0.24904921650886536, accuracy: 0.9091796875\n",
      "batch 180 loss: 0.20701910555362701, accuracy: 0.919921875\n",
      "training loss: 0.23551247790455818, accuracy: 0.9154424090877242\n",
      "validation loss: 0.23890434205532074, accuracy: 0.9153181676047529\n",
      "batch 0 loss: 0.2659056782722473, accuracy: 0.9140625\n",
      "batch 20 loss: 0.2465386539697647, accuracy: 0.91796875\n",
      "batch 40 loss: 0.2487114667892456, accuracy: 0.912109375\n",
      "batch 60 loss: 0.24876448512077332, accuracy: 0.908203125\n",
      "batch 80 loss: 0.27942073345184326, accuracy: 0.890625\n",
      "batch 100 loss: 0.23441007733345032, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.22195249795913696, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.2528611123561859, accuracy: 0.8994140625\n",
      "batch 160 loss: 0.2490788996219635, accuracy: 0.908203125\n",
      "batch 180 loss: 0.20683211088180542, accuracy: 0.919921875\n",
      "training loss: 0.2351825587451458, accuracy: 0.9154424090877242\n",
      "validation loss: 0.2381022721529007, accuracy: 0.91541588492808\n",
      "batch 0 loss: 0.2651228904724121, accuracy: 0.9140625\n",
      "batch 20 loss: 0.2454940378665924, accuracy: 0.91796875\n",
      "batch 40 loss: 0.24852417409420013, accuracy: 0.912109375\n",
      "batch 60 loss: 0.24881407618522644, accuracy: 0.9091796875\n",
      "batch 80 loss: 0.2787497639656067, accuracy: 0.8916015625\n",
      "batch 100 loss: 0.23393681645393372, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.22203156352043152, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.2538291811943054, accuracy: 0.8994140625\n",
      "batch 160 loss: 0.24796628952026367, accuracy: 0.90625\n",
      "batch 180 loss: 0.20681998133659363, accuracy: 0.919921875\n",
      "training loss: 0.23489824630320072, accuracy: 0.9155010028377242\n",
      "validation loss: 0.2370859682559967, accuracy: 0.915552689180738\n",
      "batch 0 loss: 0.2640840411186218, accuracy: 0.9140625\n",
      "batch 20 loss: 0.2438487410545349, accuracy: 0.91796875\n",
      "batch 40 loss: 0.2481972724199295, accuracy: 0.912109375\n",
      "batch 60 loss: 0.24869754910469055, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2792746424674988, accuracy: 0.890625\n",
      "batch 100 loss: 0.2337886393070221, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.22217047214508057, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.25511541962623596, accuracy: 0.8984375\n",
      "batch 160 loss: 0.24743804335594177, accuracy: 0.90625\n",
      "batch 180 loss: 0.206507608294487, accuracy: 0.919921875\n",
      "training loss: 0.23459603846073152, accuracy: 0.9155554363438901\n",
      "validation loss: 0.2365318387746811, accuracy: 0.9155917761100688\n",
      "batch 0 loss: 0.2635766863822937, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.243627667427063, accuracy: 0.91796875\n",
      "batch 40 loss: 0.24789828062057495, accuracy: 0.912109375\n",
      "batch 60 loss: 0.24854716658592224, accuracy: 0.9091796875\n",
      "batch 80 loss: 0.2783828377723694, accuracy: 0.8916015625\n",
      "batch 100 loss: 0.23223434388637543, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.22167202830314636, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.25456503033638, accuracy: 0.8984375\n",
      "batch 160 loss: 0.24718400835990906, accuracy: 0.90625\n",
      "batch 180 loss: 0.2064494788646698, accuracy: 0.9189453125\n",
      "training loss: 0.23428255796432496, accuracy: 0.9155163738438901\n",
      "validation loss: 0.23587064445018768, accuracy: 0.9155331457160726\n",
      "batch 0 loss: 0.262972354888916, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.24298110604286194, accuracy: 0.91796875\n",
      "batch 40 loss: 0.24756371974945068, accuracy: 0.912109375\n",
      "batch 60 loss: 0.248544380068779, accuracy: 0.9091796875\n",
      "batch 80 loss: 0.27836883068084717, accuracy: 0.8916015625\n",
      "batch 100 loss: 0.23170609772205353, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.22194591164588928, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.2545458674430847, accuracy: 0.8984375\n",
      "batch 160 loss: 0.24616438150405884, accuracy: 0.9072265625\n",
      "batch 180 loss: 0.20592308044433594, accuracy: 0.9189453125\n",
      "training loss: 0.2340131837129593, accuracy: 0.9155163738438901\n",
      "validation loss: 0.2357194423675537, accuracy: 0.915552689180738\n",
      "batch 0 loss: 0.2630300521850586, accuracy: 0.912109375\n",
      "batch 20 loss: 0.24330538511276245, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.2477453351020813, accuracy: 0.912109375\n",
      "batch 60 loss: 0.24832172691822052, accuracy: 0.9091796875\n",
      "batch 80 loss: 0.27807196974754333, accuracy: 0.8916015625\n",
      "batch 100 loss: 0.23109470307826996, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.22242280840873718, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.25527435541152954, accuracy: 0.8984375\n",
      "batch 160 loss: 0.245152086019516, accuracy: 0.91015625\n",
      "batch 180 loss: 0.20572513341903687, accuracy: 0.9189453125\n",
      "training loss: 0.2338214547932148, accuracy: 0.9155505535313901\n",
      "validation loss: 0.23580586910247803, accuracy: 0.9154745153220762\n",
      "batch 0 loss: 0.26325851678848267, accuracy: 0.912109375\n",
      "batch 20 loss: 0.2423035353422165, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.2476704865694046, accuracy: 0.912109375\n",
      "batch 60 loss: 0.24833017587661743, accuracy: 0.9091796875\n",
      "batch 80 loss: 0.277669757604599, accuracy: 0.892578125\n",
      "batch 100 loss: 0.2309032678604126, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.22227013111114502, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.2547859251499176, accuracy: 0.8984375\n",
      "batch 160 loss: 0.24494335055351257, accuracy: 0.91015625\n",
      "batch 180 loss: 0.20555812120437622, accuracy: 0.9189453125\n",
      "training loss: 0.23365428768098354, accuracy: 0.9155407879063902\n",
      "validation loss: 0.23559382557868958, accuracy: 0.9154354283927455\n",
      "batch 0 loss: 0.2631448805332184, accuracy: 0.912109375\n",
      "batch 20 loss: 0.24019885063171387, accuracy: 0.9169921875\n",
      "batch 40 loss: 0.24751858413219452, accuracy: 0.912109375\n",
      "batch 60 loss: 0.24808798730373383, accuracy: 0.9091796875\n",
      "batch 80 loss: 0.2777347266674042, accuracy: 0.892578125\n",
      "batch 100 loss: 0.2306038737297058, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.22267889976501465, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.25496095418930054, accuracy: 0.8984375\n",
      "batch 160 loss: 0.24347400665283203, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20546451210975647, accuracy: 0.9189453125\n",
      "training loss: 0.23331110693514348, accuracy: 0.9155359050938902\n",
      "validation loss: 0.23458954691886902, accuracy: 0.9154549718574109\n",
      "batch 0 loss: 0.26211443543434143, accuracy: 0.912109375\n",
      "batch 20 loss: 0.23902441561222076, accuracy: 0.91796875\n",
      "batch 40 loss: 0.24684806168079376, accuracy: 0.912109375\n",
      "batch 60 loss: 0.24810007214546204, accuracy: 0.9091796875\n",
      "batch 80 loss: 0.277576744556427, accuracy: 0.8916015625\n",
      "batch 100 loss: 0.23008942604064941, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.22287216782569885, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.2546502947807312, accuracy: 0.8994140625\n",
      "batch 160 loss: 0.24324774742126465, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20570367574691772, accuracy: 0.9189453125\n",
      "training loss: 0.23309093818068505, accuracy: 0.9156335613438901\n",
      "validation loss: 0.23340317606925964, accuracy: 0.9155331457160726\n",
      "batch 0 loss: 0.26078593730926514, accuracy: 0.912109375\n",
      "batch 20 loss: 0.23899798095226288, accuracy: 0.91796875\n",
      "batch 40 loss: 0.24741336703300476, accuracy: 0.912109375\n",
      "batch 60 loss: 0.24782049655914307, accuracy: 0.9091796875\n",
      "batch 80 loss: 0.27766358852386475, accuracy: 0.8916015625\n",
      "batch 100 loss: 0.2297244668006897, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.2225390076637268, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.25461724400520325, accuracy: 0.8984375\n",
      "batch 160 loss: 0.24206121265888214, accuracy: 0.9140625\n",
      "batch 180 loss: 0.2056308090686798, accuracy: 0.9189453125\n",
      "training loss: 0.23285607375204564, accuracy: 0.9156677410313901\n",
      "validation loss: 0.23296736180782318, accuracy: 0.915552689180738\n",
      "batch 0 loss: 0.26026877760887146, accuracy: 0.912109375\n",
      "batch 20 loss: 0.23851460218429565, accuracy: 0.9169921875\n",
      "batch 40 loss: 0.24716782569885254, accuracy: 0.912109375\n",
      "batch 60 loss: 0.24788975715637207, accuracy: 0.9091796875\n",
      "batch 80 loss: 0.277868390083313, accuracy: 0.8896484375\n",
      "batch 100 loss: 0.2297298014163971, accuracy: 0.912109375\n",
      "batch 120 loss: 0.2224583923816681, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.2542523145675659, accuracy: 0.8984375\n",
      "batch 160 loss: 0.2431551218032837, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20575480163097382, accuracy: 0.9189453125\n",
      "training loss: 0.23272381201386452, accuracy: 0.9156335613438901\n",
      "validation loss: 0.23257973790168762, accuracy: 0.915552689180738\n",
      "batch 0 loss: 0.2598670423030853, accuracy: 0.912109375\n",
      "batch 20 loss: 0.2379559576511383, accuracy: 0.9169921875\n",
      "batch 40 loss: 0.24671730399131775, accuracy: 0.912109375\n",
      "batch 60 loss: 0.24781206250190735, accuracy: 0.9091796875\n",
      "batch 80 loss: 0.27795544266700745, accuracy: 0.8896484375\n",
      "batch 100 loss: 0.22909265756607056, accuracy: 0.912109375\n",
      "batch 120 loss: 0.2226182520389557, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.2534358501434326, accuracy: 0.8984375\n",
      "batch 160 loss: 0.24283650517463684, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20585937798023224, accuracy: 0.9189453125\n",
      "training loss: 0.23249041490256786, accuracy: 0.9156635807875559\n",
      "validation loss: 0.23169608414173126, accuracy: 0.9157090368980613\n",
      "batch 0 loss: 0.25893324613571167, accuracy: 0.912109375\n",
      "batch 20 loss: 0.23827849328517914, accuracy: 0.9169921875\n",
      "batch 40 loss: 0.24650833010673523, accuracy: 0.912109375\n",
      "batch 60 loss: 0.2475542277097702, accuracy: 0.9091796875\n",
      "batch 80 loss: 0.277815043926239, accuracy: 0.8896484375\n",
      "batch 100 loss: 0.2290189266204834, accuracy: 0.912109375\n",
      "batch 120 loss: 0.22237688302993774, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.25356391072273254, accuracy: 0.8984375\n",
      "batch 160 loss: 0.24264192581176758, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20546917617321014, accuracy: 0.9189453125\n",
      "training loss: 0.23232994981110097, accuracy: 0.915624518287556\n",
      "validation loss: 0.23163668811321259, accuracy: 0.9157285803627268\n",
      "batch 0 loss: 0.2589351236820221, accuracy: 0.912109375\n",
      "batch 20 loss: 0.23754315078258514, accuracy: 0.9169921875\n",
      "batch 40 loss: 0.24609503149986267, accuracy: 0.912109375\n",
      "batch 60 loss: 0.247633695602417, accuracy: 0.9091796875\n",
      "batch 80 loss: 0.2775980830192566, accuracy: 0.8916015625\n",
      "batch 100 loss: 0.22844183444976807, accuracy: 0.912109375\n",
      "batch 120 loss: 0.22226709127426147, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.2530561089515686, accuracy: 0.8984375\n",
      "batch 160 loss: 0.24334289133548737, accuracy: 0.9111328125\n",
      "batch 180 loss: 0.20559513568878174, accuracy: 0.919921875\n",
      "training loss: 0.23220867924392224, accuracy: 0.915697760475056\n",
      "validation loss: 0.23157073557376862, accuracy: 0.9157090368980613\n",
      "batch 0 loss: 0.2588788568973541, accuracy: 0.912109375\n",
      "batch 20 loss: 0.23656634986400604, accuracy: 0.91796875\n",
      "batch 40 loss: 0.2460494488477707, accuracy: 0.912109375\n",
      "batch 60 loss: 0.24746444821357727, accuracy: 0.9091796875\n",
      "batch 80 loss: 0.27717697620391846, accuracy: 0.892578125\n",
      "batch 100 loss: 0.2280896157026291, accuracy: 0.912109375\n",
      "batch 120 loss: 0.2225019633769989, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.2534940242767334, accuracy: 0.8984375\n",
      "batch 160 loss: 0.24098095297813416, accuracy: 0.9140625\n",
      "batch 180 loss: 0.20554295182228088, accuracy: 0.919921875\n",
      "training loss: 0.23194097578525544, accuracy: 0.915707526100056\n",
      "validation loss: 0.23153650760650635, accuracy: 0.9157090368980613\n",
      "batch 0 loss: 0.25895631313323975, accuracy: 0.912109375\n",
      "batch 20 loss: 0.23659369349479675, accuracy: 0.9169921875\n",
      "batch 40 loss: 0.24599964916706085, accuracy: 0.912109375\n",
      "batch 60 loss: 0.24735957384109497, accuracy: 0.9091796875\n",
      "batch 80 loss: 0.2771487832069397, accuracy: 0.892578125\n",
      "batch 100 loss: 0.22802802920341492, accuracy: 0.912109375\n",
      "batch 120 loss: 0.22255577147006989, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.25355207920074463, accuracy: 0.8984375\n",
      "batch 160 loss: 0.24086439609527588, accuracy: 0.9140625\n",
      "batch 180 loss: 0.20540645718574524, accuracy: 0.919921875\n",
      "training loss: 0.23177859030663966, accuracy: 0.9156635807875559\n",
      "validation loss: 0.23139019310474396, accuracy: 0.9157676672920575\n",
      "batch 0 loss: 0.2588617503643036, accuracy: 0.912109375\n",
      "batch 20 loss: 0.23646961152553558, accuracy: 0.9169921875\n",
      "batch 40 loss: 0.2461261749267578, accuracy: 0.912109375\n",
      "batch 60 loss: 0.24741530418395996, accuracy: 0.9091796875\n",
      "batch 80 loss: 0.276985228061676, accuracy: 0.892578125\n",
      "batch 100 loss: 0.22759366035461426, accuracy: 0.912109375\n",
      "batch 120 loss: 0.2218906581401825, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.2534957230091095, accuracy: 0.8984375\n",
      "batch 160 loss: 0.24125970900058746, accuracy: 0.9140625\n",
      "batch 180 loss: 0.20539703965187073, accuracy: 0.9208984375\n",
      "training loss: 0.23168968878686427, accuracy: 0.915639166725056\n",
      "validation loss: 0.2311885505914688, accuracy: 0.9157090368980613\n",
      "batch 0 loss: 0.2587931156158447, accuracy: 0.912109375\n",
      "batch 20 loss: 0.23549288511276245, accuracy: 0.91796875\n",
      "batch 40 loss: 0.24610857665538788, accuracy: 0.912109375\n",
      "batch 60 loss: 0.24720005691051483, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2766912579536438, accuracy: 0.892578125\n",
      "batch 100 loss: 0.22726638615131378, accuracy: 0.912109375\n",
      "batch 120 loss: 0.22166794538497925, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.25291383266448975, accuracy: 0.8984375\n",
      "batch 160 loss: 0.24052780866622925, accuracy: 0.9140625\n",
      "batch 180 loss: 0.20521071553230286, accuracy: 0.9208984375\n",
      "training loss: 0.23145813390612602, accuracy: 0.915697760475056\n",
      "validation loss: 0.23069654405117035, accuracy: 0.915787210756723\n",
      "batch 0 loss: 0.2582736611366272, accuracy: 0.912109375\n",
      "batch 20 loss: 0.23523227870464325, accuracy: 0.91796875\n",
      "batch 40 loss: 0.24672305583953857, accuracy: 0.912109375\n",
      "batch 60 loss: 0.24721811711788177, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27619993686676025, accuracy: 0.892578125\n",
      "batch 100 loss: 0.22687658667564392, accuracy: 0.912109375\n",
      "batch 120 loss: 0.2214449644088745, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.25318801403045654, accuracy: 0.8974609375\n",
      "batch 160 loss: 0.24077965319156647, accuracy: 0.9140625\n",
      "batch 180 loss: 0.20545414090156555, accuracy: 0.9208984375\n",
      "training loss: 0.23134819582104682, accuracy: 0.915746588600056\n",
      "validation loss: 0.23068945109844208, accuracy: 0.915787210756723\n",
      "batch 0 loss: 0.2582855224609375, accuracy: 0.912109375\n",
      "batch 20 loss: 0.23359575867652893, accuracy: 0.91796875\n",
      "batch 40 loss: 0.24553899466991425, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.2472197711467743, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2770311236381531, accuracy: 0.892578125\n",
      "batch 100 loss: 0.22685131430625916, accuracy: 0.912109375\n",
      "batch 120 loss: 0.2213606834411621, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.25313708186149597, accuracy: 0.896484375\n",
      "batch 160 loss: 0.2391154021024704, accuracy: 0.9140625\n",
      "batch 180 loss: 0.20528171956539154, accuracy: 0.9208984375\n",
      "training loss: 0.23114894166588784, accuracy: 0.915707526100056\n",
      "validation loss: 0.2305247187614441, accuracy: 0.9159435584740463\n",
      "batch 0 loss: 0.2583940029144287, accuracy: 0.912109375\n",
      "batch 20 loss: 0.2332468330860138, accuracy: 0.91796875\n",
      "batch 40 loss: 0.24560466408729553, accuracy: 0.912109375\n",
      "batch 60 loss: 0.24723608791828156, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27678775787353516, accuracy: 0.892578125\n",
      "batch 100 loss: 0.22675791382789612, accuracy: 0.912109375\n",
      "batch 120 loss: 0.22087591886520386, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.25311577320098877, accuracy: 0.896484375\n",
      "batch 160 loss: 0.24214032292366028, accuracy: 0.9111328125\n",
      "batch 180 loss: 0.20562142133712769, accuracy: 0.919921875\n",
      "training loss: 0.23109346196055414, accuracy: 0.915712408912556\n",
      "validation loss: 0.2304491251707077, accuracy: 0.9158653846153846\n",
      "batch 0 loss: 0.258197158575058, accuracy: 0.912109375\n",
      "batch 20 loss: 0.23273032903671265, accuracy: 0.91796875\n",
      "batch 40 loss: 0.24568244814872742, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.2473093569278717, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2767750322818756, accuracy: 0.892578125\n",
      "batch 100 loss: 0.2263883799314499, accuracy: 0.912109375\n",
      "batch 120 loss: 0.22082814574241638, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.2524755597114563, accuracy: 0.8974609375\n",
      "batch 160 loss: 0.24090944230556488, accuracy: 0.9140625\n",
      "batch 180 loss: 0.20548588037490845, accuracy: 0.919921875\n",
      "training loss: 0.2309278894215822, accuracy: 0.915741705787556\n",
      "validation loss: 0.2303045094013214, accuracy: 0.9159240150093808\n",
      "batch 0 loss: 0.25832289457321167, accuracy: 0.912109375\n",
      "batch 20 loss: 0.2325633317232132, accuracy: 0.91796875\n",
      "batch 40 loss: 0.24536670744419098, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.2472752034664154, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2767261862754822, accuracy: 0.892578125\n",
      "batch 100 loss: 0.2262861132621765, accuracy: 0.912109375\n",
      "batch 120 loss: 0.2205832600593567, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.25285932421684265, accuracy: 0.896484375\n",
      "batch 160 loss: 0.24069084227085114, accuracy: 0.9140625\n",
      "batch 180 loss: 0.20537398755550385, accuracy: 0.919921875\n",
      "training loss: 0.23079483538866044, accuracy: 0.915766119850056\n",
      "validation loss: 0.2301783412694931, accuracy: 0.9159631019387117\n",
      "batch 0 loss: 0.25809919834136963, accuracy: 0.912109375\n",
      "batch 20 loss: 0.23205941915512085, accuracy: 0.91796875\n",
      "batch 40 loss: 0.24485798180103302, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24721501767635345, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2765345871448517, accuracy: 0.892578125\n",
      "batch 100 loss: 0.22589269280433655, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.22084632515907288, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.2526244521141052, accuracy: 0.8974609375\n",
      "batch 160 loss: 0.2399633228778839, accuracy: 0.9140625\n",
      "batch 180 loss: 0.2053631842136383, accuracy: 0.919921875\n",
      "training loss: 0.23067535892128943, accuracy: 0.915800299537556\n",
      "validation loss: 0.22979387640953064, accuracy: 0.9160021888680425\n",
      "batch 0 loss: 0.25778821110725403, accuracy: 0.912109375\n",
      "batch 20 loss: 0.2318263053894043, accuracy: 0.91796875\n",
      "batch 40 loss: 0.24532243609428406, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24722854793071747, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.2764584422111511, accuracy: 0.892578125\n",
      "batch 100 loss: 0.22569337487220764, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.22083234786987305, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.2527327537536621, accuracy: 0.896484375\n",
      "batch 160 loss: 0.23957225680351257, accuracy: 0.9140625\n",
      "batch 180 loss: 0.20521999895572662, accuracy: 0.919921875\n",
      "training loss: 0.2306094017624855, accuracy: 0.915771002662556\n",
      "validation loss: 0.22973494231700897, accuracy: 0.9160608192620388\n",
      "batch 0 loss: 0.2577892541885376, accuracy: 0.912109375\n",
      "batch 20 loss: 0.23182156682014465, accuracy: 0.91796875\n",
      "batch 40 loss: 0.24578657746315002, accuracy: 0.912109375\n",
      "batch 60 loss: 0.24727222323417664, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.27617210149765015, accuracy: 0.892578125\n",
      "batch 100 loss: 0.22505810856819153, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.22057181596755981, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.2523001432418823, accuracy: 0.8984375\n",
      "batch 160 loss: 0.23987790942192078, accuracy: 0.9140625\n",
      "batch 180 loss: 0.2051888257265091, accuracy: 0.919921875\n",
      "training loss: 0.23052705354988576, accuracy: 0.915810065162556\n",
      "validation loss: 0.22967837750911713, accuracy: 0.9159826454033771\n",
      "batch 0 loss: 0.25775519013404846, accuracy: 0.912109375\n",
      "batch 20 loss: 0.23182463645935059, accuracy: 0.91796875\n",
      "batch 40 loss: 0.24519354104995728, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24737954139709473, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.27659255266189575, accuracy: 0.8916015625\n",
      "batch 100 loss: 0.22487583756446838, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.22047241032123566, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.25239986181259155, accuracy: 0.8974609375\n",
      "batch 160 loss: 0.23913027346134186, accuracy: 0.9140625\n",
      "batch 180 loss: 0.2054627388715744, accuracy: 0.919921875\n",
      "training loss: 0.23043859094381333, accuracy: 0.915814947975056\n",
      "validation loss: 0.2292490154504776, accuracy: 0.9160608192620388\n",
      "batch 0 loss: 0.257220059633255, accuracy: 0.912109375\n",
      "batch 20 loss: 0.2311391979455948, accuracy: 0.91796875\n",
      "batch 40 loss: 0.2460331916809082, accuracy: 0.912109375\n",
      "batch 60 loss: 0.2473309338092804, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.27644431591033936, accuracy: 0.892578125\n",
      "batch 100 loss: 0.2246047705411911, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.22041985392570496, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.25216326117515564, accuracy: 0.8974609375\n",
      "batch 160 loss: 0.23919552564620972, accuracy: 0.9140625\n",
      "batch 180 loss: 0.20547667145729065, accuracy: 0.919921875\n",
      "training loss: 0.2303280896693468, accuracy: 0.915834479225056\n",
      "validation loss: 0.22920364141464233, accuracy: 0.9160608192620388\n",
      "batch 0 loss: 0.25734567642211914, accuracy: 0.912109375\n",
      "batch 20 loss: 0.23137196898460388, accuracy: 0.91796875\n",
      "batch 40 loss: 0.2454718053340912, accuracy: 0.912109375\n",
      "batch 60 loss: 0.24752436578273773, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.2763141393661499, accuracy: 0.892578125\n",
      "batch 100 loss: 0.22453169524669647, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.22052234411239624, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.2523224949836731, accuracy: 0.8974609375\n",
      "batch 160 loss: 0.2390095740556717, accuracy: 0.9140625\n",
      "batch 180 loss: 0.2057313472032547, accuracy: 0.919921875\n",
      "training loss: 0.2302680554240942, accuracy: 0.915854010475056\n",
      "validation loss: 0.2290596216917038, accuracy: 0.9160999061913696\n",
      "batch 0 loss: 0.25723767280578613, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.23104946315288544, accuracy: 0.91796875\n",
      "batch 40 loss: 0.24495618045330048, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24759404361248016, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.27636492252349854, accuracy: 0.892578125\n",
      "batch 100 loss: 0.22432515025138855, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.22007392346858978, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.25161951780319214, accuracy: 0.8974609375\n",
      "batch 160 loss: 0.23878663778305054, accuracy: 0.9140625\n",
      "batch 180 loss: 0.20557406544685364, accuracy: 0.919921875\n",
      "training loss: 0.2301066642999649, accuracy: 0.9159126042250559\n",
      "validation loss: 0.22892816364765167, accuracy: 0.9160803627267042\n",
      "batch 0 loss: 0.25704875588417053, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.2306259274482727, accuracy: 0.91796875\n",
      "batch 40 loss: 0.24487075209617615, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24752399325370789, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.27640923857688904, accuracy: 0.8916015625\n",
      "batch 100 loss: 0.2243310511112213, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.22008971869945526, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.25236010551452637, accuracy: 0.8974609375\n",
      "batch 160 loss: 0.23904389142990112, accuracy: 0.9130859375\n",
      "batch 180 loss: 0.20572954416275024, accuracy: 0.919921875\n",
      "training loss: 0.23002305269241333, accuracy: 0.915883307350056\n",
      "validation loss: 0.2284090667963028, accuracy: 0.916119449656035\n",
      "batch 0 loss: 0.2564661204814911, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22984355688095093, accuracy: 0.91796875\n",
      "batch 40 loss: 0.24493470788002014, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24760940670967102, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.2760545611381531, accuracy: 0.892578125\n",
      "batch 100 loss: 0.2241894155740738, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.21971145272254944, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.2518084645271301, accuracy: 0.8974609375\n",
      "batch 160 loss: 0.23851971328258514, accuracy: 0.9140625\n",
      "batch 180 loss: 0.20580795407295227, accuracy: 0.919921875\n",
      "training loss: 0.2299160635471344, accuracy: 0.915902838600056\n",
      "validation loss: 0.2284659594297409, accuracy: 0.9160608192620388\n",
      "batch 0 loss: 0.25654006004333496, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22991305589675903, accuracy: 0.91796875\n",
      "batch 40 loss: 0.24513037502765656, accuracy: 0.912109375\n",
      "batch 60 loss: 0.24772407114505768, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.27592015266418457, accuracy: 0.892578125\n",
      "batch 100 loss: 0.22426864504814148, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.21951612830162048, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.251433789730072, accuracy: 0.8974609375\n",
      "batch 160 loss: 0.2383124977350235, accuracy: 0.9140625\n",
      "batch 180 loss: 0.20566269755363464, accuracy: 0.9208984375\n",
      "training loss: 0.22983772799372673, accuracy: 0.915868658912556\n",
      "validation loss: 0.22848305106163025, accuracy: 0.916119449656035\n",
      "batch 0 loss: 0.25674575567245483, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22966371476650238, accuracy: 0.91796875\n",
      "batch 40 loss: 0.24506330490112305, accuracy: 0.912109375\n",
      "batch 60 loss: 0.24760103225708008, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.2760249376296997, accuracy: 0.892578125\n",
      "batch 100 loss: 0.22410166263580322, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.2192825824022293, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.2517770528793335, accuracy: 0.8984375\n",
      "batch 160 loss: 0.238602876663208, accuracy: 0.9130859375\n",
      "batch 180 loss: 0.20574229955673218, accuracy: 0.919921875\n",
      "training loss: 0.22973152078688144, accuracy: 0.9159174870375559\n",
      "validation loss: 0.22808894515037537, accuracy: 0.9160999061913696\n",
      "batch 0 loss: 0.25625017285346985, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22899752855300903, accuracy: 0.91796875\n",
      "batch 40 loss: 0.24410760402679443, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.2476673126220703, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.27582114934921265, accuracy: 0.892578125\n",
      "batch 100 loss: 0.2238864004611969, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.2191726416349411, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.2517247200012207, accuracy: 0.8974609375\n",
      "batch 160 loss: 0.2383963018655777, accuracy: 0.9140625\n",
      "batch 180 loss: 0.20588418841362, accuracy: 0.9208984375\n",
      "training loss: 0.22964106567203998, accuracy: 0.915873541725056\n",
      "validation loss: 0.22801920771598816, accuracy: 0.916119449656035\n",
      "batch 0 loss: 0.2561986446380615, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.2290499210357666, accuracy: 0.91796875\n",
      "batch 40 loss: 0.2444162368774414, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24780423939228058, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.2756560444831848, accuracy: 0.892578125\n",
      "batch 100 loss: 0.2238263189792633, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.21919351816177368, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.2517631947994232, accuracy: 0.8974609375\n",
      "batch 160 loss: 0.23827292025089264, accuracy: 0.9140625\n",
      "batch 180 loss: 0.2059631049633026, accuracy: 0.921875\n",
      "training loss: 0.22959030754864215, accuracy: 0.915883307350056\n",
      "validation loss: 0.22798004746437073, accuracy: 0.9161976235146967\n",
      "batch 0 loss: 0.25620532035827637, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22907400131225586, accuracy: 0.91796875\n",
      "batch 40 loss: 0.24470773339271545, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24780166149139404, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.27578845620155334, accuracy: 0.892578125\n",
      "batch 100 loss: 0.22371622920036316, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.21881064772605896, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.2514726221561432, accuracy: 0.8984375\n",
      "batch 160 loss: 0.23793551325798035, accuracy: 0.9140625\n",
      "batch 180 loss: 0.20588542520999908, accuracy: 0.9208984375\n",
      "training loss: 0.22952751606702804, accuracy: 0.9159133267937221\n",
      "validation loss: 0.22770071029663086, accuracy: 0.9161780800500313\n",
      "batch 0 loss: 0.25592994689941406, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22879472374916077, accuracy: 0.91796875\n",
      "batch 40 loss: 0.2444498986005783, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24803230166435242, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.2759338617324829, accuracy: 0.892578125\n",
      "batch 100 loss: 0.2237270176410675, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.2186242938041687, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.2513532042503357, accuracy: 0.8984375\n",
      "batch 160 loss: 0.23786890506744385, accuracy: 0.9140625\n",
      "batch 180 loss: 0.2060425877571106, accuracy: 0.921875\n",
      "training loss: 0.22939623922109603, accuracy: 0.915941901100056\n",
      "validation loss: 0.22763554751873016, accuracy: 0.9161976235146967\n",
      "batch 0 loss: 0.2559434473514557, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22818368673324585, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24444448947906494, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24806468188762665, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.2759077548980713, accuracy: 0.892578125\n",
      "batch 100 loss: 0.22386278212070465, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.2185964286327362, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.25176146626472473, accuracy: 0.8984375\n",
      "batch 160 loss: 0.2374112904071808, accuracy: 0.9140625\n",
      "batch 180 loss: 0.20560650527477264, accuracy: 0.9208984375\n",
      "training loss: 0.2293410074710846, accuracy: 0.915902838600056\n",
      "validation loss: 0.227851003408432, accuracy: 0.9162171669793621\n",
      "batch 0 loss: 0.25630155205726624, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22844351828098297, accuracy: 0.91796875\n",
      "batch 40 loss: 0.24445149302482605, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24777472019195557, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2758476138114929, accuracy: 0.892578125\n",
      "batch 100 loss: 0.22368337213993073, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.21835854649543762, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.25167256593704224, accuracy: 0.8984375\n",
      "batch 160 loss: 0.23801523447036743, accuracy: 0.9130859375\n",
      "batch 180 loss: 0.20577332377433777, accuracy: 0.921875\n",
      "training loss: 0.2292859172075987, accuracy: 0.915879147106222\n",
      "validation loss: 0.2277825027704239, accuracy: 0.9161976235146967\n",
      "batch 0 loss: 0.25612154603004456, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22804993391036987, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24446944892406464, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24781233072280884, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27593865990638733, accuracy: 0.892578125\n",
      "batch 100 loss: 0.2236001193523407, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.21837559342384338, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.2515415847301483, accuracy: 0.8984375\n",
      "batch 160 loss: 0.23776209354400635, accuracy: 0.9130859375\n",
      "batch 180 loss: 0.20588025450706482, accuracy: 0.9228515625\n",
      "training loss: 0.2292365761846304, accuracy: 0.915927975231222\n",
      "validation loss: 0.2276683747768402, accuracy: 0.9161585365853658\n",
      "batch 0 loss: 0.25599563121795654, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.2280043363571167, accuracy: 0.91796875\n",
      "batch 40 loss: 0.2442193329334259, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24776645004749298, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2757090926170349, accuracy: 0.892578125\n",
      "batch 100 loss: 0.22334566712379456, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.218161940574646, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.2510887682437897, accuracy: 0.8984375\n",
      "batch 160 loss: 0.23794744908809662, accuracy: 0.9130859375\n",
      "batch 180 loss: 0.20597997307777405, accuracy: 0.9228515625\n",
      "training loss: 0.22913131900131703, accuracy: 0.915932858043722\n",
      "validation loss: 0.22759298980236053, accuracy: 0.9160999061913696\n",
      "batch 0 loss: 0.2560074031352997, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22768136858940125, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24438825249671936, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24791836738586426, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.27579718828201294, accuracy: 0.892578125\n",
      "batch 100 loss: 0.22334223985671997, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.21824543178081512, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.2519904375076294, accuracy: 0.8984375\n",
      "batch 160 loss: 0.23848524689674377, accuracy: 0.9111328125\n",
      "batch 180 loss: 0.20572742819786072, accuracy: 0.921875\n",
      "training loss: 0.22909631200134753, accuracy: 0.9159133267937221\n",
      "validation loss: 0.22738710045814514, accuracy: 0.9161976235146967\n",
      "batch 0 loss: 0.2558174133300781, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22693948447704315, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.2442978322505951, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24807143211364746, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.27591201663017273, accuracy: 0.8916015625\n",
      "batch 100 loss: 0.22312352061271667, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.21816202998161316, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.2511335015296936, accuracy: 0.8994140625\n",
      "batch 160 loss: 0.238042414188385, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20622433722019196, accuracy: 0.9228515625\n",
      "training loss: 0.2290162218362093, accuracy: 0.916006100231222\n",
      "validation loss: 0.2272203266620636, accuracy: 0.9161976235146967\n",
      "batch 0 loss: 0.2556568384170532, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22683435678482056, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24396690726280212, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24805335700511932, accuracy: 0.91015625\n",
      "batch 80 loss: 0.275675892829895, accuracy: 0.892578125\n",
      "batch 100 loss: 0.2231139838695526, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.2180073857307434, accuracy: 0.919921875\n",
      "batch 140 loss: 0.25097957253456116, accuracy: 0.8994140625\n",
      "batch 160 loss: 0.23788996040821075, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20633628964424133, accuracy: 0.9228515625\n",
      "training loss: 0.22893573246896268, accuracy: 0.915971197975056\n",
      "validation loss: 0.22705867886543274, accuracy: 0.9161976235146967\n",
      "batch 0 loss: 0.25550004839897156, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22707392275333405, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24400489032268524, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24808897078037262, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2756291627883911, accuracy: 0.8935546875\n",
      "batch 100 loss: 0.22300173342227936, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.21795596182346344, accuracy: 0.919921875\n",
      "batch 140 loss: 0.2514037489891052, accuracy: 0.8994140625\n",
      "batch 160 loss: 0.23704180121421814, accuracy: 0.9130859375\n",
      "batch 180 loss: 0.2064620554447174, accuracy: 0.9228515625\n",
      "training loss: 0.2289098057895899, accuracy: 0.916010260475056\n",
      "validation loss: 0.22685736417770386, accuracy: 0.9162562539086929\n",
      "batch 0 loss: 0.25526994466781616, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22687654197216034, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24404306709766388, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24801748991012573, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2757059633731842, accuracy: 0.892578125\n",
      "batch 100 loss: 0.22286097705364227, accuracy: 0.912109375\n",
      "batch 120 loss: 0.2180069386959076, accuracy: 0.9189453125\n",
      "batch 140 loss: 0.25085845589637756, accuracy: 0.8994140625\n",
      "batch 160 loss: 0.23745177686214447, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20657026767730713, accuracy: 0.9228515625\n",
      "training loss: 0.22884843848645686, accuracy: 0.915995612037556\n",
      "validation loss: 0.22676658630371094, accuracy: 0.9162562539086929\n",
      "batch 0 loss: 0.25525110960006714, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22640731930732727, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.2442123144865036, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24798046052455902, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27540087699890137, accuracy: 0.8935546875\n",
      "batch 100 loss: 0.22286926209926605, accuracy: 0.912109375\n",
      "batch 120 loss: 0.21798962354660034, accuracy: 0.919921875\n",
      "batch 140 loss: 0.25065991282463074, accuracy: 0.8994140625\n",
      "batch 160 loss: 0.23748356103897095, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20612716674804688, accuracy: 0.9228515625\n",
      "training loss: 0.22877947755157949, accuracy: 0.916001217418722\n",
      "validation loss: 0.2265852391719818, accuracy: 0.9161780800500313\n",
      "batch 0 loss: 0.2550152540206909, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22659210860729218, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24402020871639252, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24810531735420227, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2753961682319641, accuracy: 0.8935546875\n",
      "batch 100 loss: 0.22256404161453247, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.21786630153656006, accuracy: 0.919921875\n",
      "batch 140 loss: 0.25077176094055176, accuracy: 0.8994140625\n",
      "batch 160 loss: 0.23718693852424622, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20607315003871918, accuracy: 0.9228515625\n",
      "training loss: 0.22872511073946952, accuracy: 0.915961432350056\n",
      "validation loss: 0.22656545042991638, accuracy: 0.9161976235146967\n",
      "batch 0 loss: 0.25506389141082764, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22636932134628296, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24353036284446716, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24804672598838806, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27544325590133667, accuracy: 0.8935546875\n",
      "batch 100 loss: 0.2228190004825592, accuracy: 0.912109375\n",
      "batch 120 loss: 0.21769922971725464, accuracy: 0.919921875\n",
      "batch 140 loss: 0.25082993507385254, accuracy: 0.8994140625\n",
      "batch 160 loss: 0.2374471127986908, accuracy: 0.9111328125\n",
      "batch 180 loss: 0.20618751645088196, accuracy: 0.9228515625\n",
      "training loss: 0.22867208905518055, accuracy: 0.915985846412556\n",
      "validation loss: 0.2264244109392166, accuracy: 0.9161780800500313\n",
      "batch 0 loss: 0.25487220287323, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.2262532114982605, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24381110072135925, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24804599583148956, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2753945291042328, accuracy: 0.892578125\n",
      "batch 100 loss: 0.2225414216518402, accuracy: 0.912109375\n",
      "batch 120 loss: 0.2174449861049652, accuracy: 0.919921875\n",
      "batch 140 loss: 0.25109097361564636, accuracy: 0.8994140625\n",
      "batch 160 loss: 0.23750966787338257, accuracy: 0.9111328125\n",
      "batch 180 loss: 0.2062029391527176, accuracy: 0.9228515625\n",
      "training loss: 0.2286229083687067, accuracy: 0.915976080787556\n",
      "validation loss: 0.22649286687374115, accuracy: 0.9161780800500313\n",
      "batch 0 loss: 0.2549722492694855, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.2262433022260666, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.2440049797296524, accuracy: 0.912109375\n",
      "batch 60 loss: 0.24825477600097656, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2752546966075897, accuracy: 0.892578125\n",
      "batch 100 loss: 0.22236573696136475, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.21757188439369202, accuracy: 0.919921875\n",
      "batch 140 loss: 0.25145381689071655, accuracy: 0.8994140625\n",
      "batch 160 loss: 0.2374396026134491, accuracy: 0.9111328125\n",
      "batch 180 loss: 0.206294447183609, accuracy: 0.9228515625\n",
      "training loss: 0.22854922369122505, accuracy: 0.916010260475056\n",
      "validation loss: 0.22643685340881348, accuracy: 0.9161389931207005\n",
      "batch 0 loss: 0.2549772262573242, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.2260361909866333, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.2439340353012085, accuracy: 0.912109375\n",
      "batch 60 loss: 0.24840626120567322, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.27532801032066345, accuracy: 0.892578125\n",
      "batch 100 loss: 0.22235730290412903, accuracy: 0.912109375\n",
      "batch 120 loss: 0.2174246907234192, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.25001415610313416, accuracy: 0.8994140625\n",
      "batch 160 loss: 0.23750178515911102, accuracy: 0.9111328125\n",
      "batch 180 loss: 0.20666630566120148, accuracy: 0.9228515625\n",
      "training loss: 0.22853611364960671, accuracy: 0.9160346745375559\n",
      "validation loss: 0.22645531594753265, accuracy: 0.9161389931207005\n",
      "batch 0 loss: 0.25496602058410645, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22593896090984344, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.2436579167842865, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.2483184039592743, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.2752528488636017, accuracy: 0.892578125\n",
      "batch 100 loss: 0.2222554236650467, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.21762949228286743, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.25002411007881165, accuracy: 0.8994140625\n",
      "batch 160 loss: 0.2373543381690979, accuracy: 0.9111328125\n",
      "batch 180 loss: 0.2062925398349762, accuracy: 0.9228515625\n",
      "training loss: 0.22847244061529637, accuracy: 0.916068854225056\n",
      "validation loss: 0.2264232486486435, accuracy: 0.9161780800500313\n",
      "batch 0 loss: 0.25491660833358765, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22579583525657654, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.2435549944639206, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.2484048455953598, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.2752092480659485, accuracy: 0.892578125\n",
      "batch 100 loss: 0.22240635752677917, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.21756044030189514, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24965834617614746, accuracy: 0.8994140625\n",
      "batch 160 loss: 0.237563818693161, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20660123229026794, accuracy: 0.9228515625\n",
      "training loss: 0.22841499350965022, accuracy: 0.916078619850056\n",
      "validation loss: 0.22629766166210175, accuracy: 0.9162367104440275\n",
      "batch 0 loss: 0.2547810673713684, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22548383474349976, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24417564272880554, accuracy: 0.912109375\n",
      "batch 60 loss: 0.2484959065914154, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.2752101719379425, accuracy: 0.892578125\n",
      "batch 100 loss: 0.22213058173656464, accuracy: 0.912109375\n",
      "batch 120 loss: 0.21737554669380188, accuracy: 0.919921875\n",
      "batch 140 loss: 0.24953389167785645, accuracy: 0.8994140625\n",
      "batch 160 loss: 0.2368893325328827, accuracy: 0.9111328125\n",
      "batch 180 loss: 0.20638608932495117, accuracy: 0.9228515625\n",
      "training loss: 0.22835813976824285, accuracy: 0.916112799537556\n",
      "validation loss: 0.22623024880886078, accuracy: 0.9162562539086929\n",
      "batch 0 loss: 0.2547546625137329, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22539514303207397, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.243568554520607, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24858540296554565, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.2752699553966522, accuracy: 0.892578125\n",
      "batch 100 loss: 0.22221365571022034, accuracy: 0.912109375\n",
      "batch 120 loss: 0.21732884645462036, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.2508684992790222, accuracy: 0.8994140625\n",
      "batch 160 loss: 0.23701800405979156, accuracy: 0.912109375\n",
      "batch 180 loss: 0.2068043053150177, accuracy: 0.923828125\n",
      "training loss: 0.22831172190606594, accuracy: 0.916083502662556\n",
      "validation loss: 0.22631047666072845, accuracy: 0.9162562539086929\n",
      "batch 0 loss: 0.2548500895500183, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22488537430763245, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24360521137714386, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24851810932159424, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.2751403748989105, accuracy: 0.8935546875\n",
      "batch 100 loss: 0.22199654579162598, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.21733097732067108, accuracy: 0.919921875\n",
      "batch 140 loss: 0.2505233883857727, accuracy: 0.8994140625\n",
      "batch 160 loss: 0.23705367743968964, accuracy: 0.912109375\n",
      "batch 180 loss: 0.2067820429801941, accuracy: 0.923828125\n",
      "training loss: 0.22824922293424607, accuracy: 0.916103033912556\n",
      "validation loss: 0.22623750567436218, accuracy: 0.9162562539086929\n",
      "batch 0 loss: 0.2547758221626282, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.2248522937297821, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24347957968711853, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24870651960372925, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.2750321626663208, accuracy: 0.8935546875\n",
      "batch 100 loss: 0.22191877663135529, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.21725612878799438, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24936912953853607, accuracy: 0.8994140625\n",
      "batch 160 loss: 0.23726841807365417, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20660457015037537, accuracy: 0.9228515625\n",
      "training loss: 0.22820101872086526, accuracy: 0.916103033912556\n",
      "validation loss: 0.22621580958366394, accuracy: 0.9162953408380238\n",
      "batch 0 loss: 0.25478246808052063, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.2251235395669937, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24397529661655426, accuracy: 0.912109375\n",
      "batch 60 loss: 0.24843338131904602, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.275115430355072, accuracy: 0.8935546875\n",
      "batch 100 loss: 0.22195768356323242, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.217235267162323, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24989882111549377, accuracy: 0.8994140625\n",
      "batch 160 loss: 0.23715731501579285, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20695586502552032, accuracy: 0.9228515625\n",
      "training loss: 0.2281816093623638, accuracy: 0.916117682350056\n",
      "validation loss: 0.22618331015110016, accuracy: 0.9162953408380238\n",
      "batch 0 loss: 0.2547120451927185, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22496244311332703, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24374711513519287, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24878434836864471, accuracy: 0.912109375\n",
      "batch 80 loss: 0.27505630254745483, accuracy: 0.8935546875\n",
      "batch 100 loss: 0.22175201773643494, accuracy: 0.9140625\n",
      "batch 120 loss: 0.2170867770910263, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24979938566684723, accuracy: 0.8994140625\n",
      "batch 160 loss: 0.23697620630264282, accuracy: 0.9130859375\n",
      "batch 180 loss: 0.2073425054550171, accuracy: 0.9228515625\n",
      "training loss: 0.228153617978096, accuracy: 0.916133053356222\n",
      "validation loss: 0.22606627643108368, accuracy: 0.9162757973733584\n",
      "batch 0 loss: 0.25465965270996094, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22468701004981995, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.2437284290790558, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24881285429000854, accuracy: 0.912109375\n",
      "batch 80 loss: 0.2751535177230835, accuracy: 0.8935546875\n",
      "batch 100 loss: 0.22183826565742493, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.21688291430473328, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24978108704090118, accuracy: 0.8994140625\n",
      "batch 160 loss: 0.2373288869857788, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20726442337036133, accuracy: 0.9228515625\n",
      "training loss: 0.22807417936623098, accuracy: 0.916112799537556\n",
      "validation loss: 0.22609196603298187, accuracy: 0.9162953408380238\n",
      "batch 0 loss: 0.25475192070007324, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.224832683801651, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.2434735894203186, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.2487790286540985, accuracy: 0.912109375\n",
      "batch 80 loss: 0.2749691605567932, accuracy: 0.8935546875\n",
      "batch 100 loss: 0.22175896167755127, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.21695199608802795, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.2494649589061737, accuracy: 0.8994140625\n",
      "batch 160 loss: 0.23685485124588013, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20719808340072632, accuracy: 0.921875\n",
      "training loss: 0.22802031449973584, accuracy: 0.9161574674187221\n",
      "validation loss: 0.22603687644004822, accuracy: 0.9163148843026891\n",
      "batch 0 loss: 0.2547850012779236, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.2247108519077301, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.2438279390335083, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.2487473487854004, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.27484774589538574, accuracy: 0.8935546875\n",
      "batch 100 loss: 0.22172008454799652, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.21698474884033203, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24952541291713715, accuracy: 0.8994140625\n",
      "batch 160 loss: 0.23665106296539307, accuracy: 0.912109375\n",
      "batch 180 loss: 0.206938698887825, accuracy: 0.9228515625\n",
      "training loss: 0.22795879684388637, accuracy: 0.916137213600056\n",
      "validation loss: 0.2260541319847107, accuracy: 0.9162953408380238\n",
      "batch 0 loss: 0.25477421283721924, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.2244529277086258, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24344128370285034, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24876466393470764, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.27489301562309265, accuracy: 0.8935546875\n",
      "batch 100 loss: 0.2215990126132965, accuracy: 0.9140625\n",
      "batch 120 loss: 0.21716004610061646, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24991478025913239, accuracy: 0.8994140625\n",
      "batch 160 loss: 0.2370164394378662, accuracy: 0.912109375\n",
      "batch 180 loss: 0.2073138952255249, accuracy: 0.921875\n",
      "training loss: 0.22799895040690898, accuracy: 0.916127447975056\n",
      "validation loss: 0.2260432243347168, accuracy: 0.9162562539086929\n",
      "batch 0 loss: 0.2548351287841797, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22464653849601746, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24352402985095978, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24869205057621002, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.2748166620731354, accuracy: 0.8935546875\n",
      "batch 100 loss: 0.2216152846813202, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.21714049577713013, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24951566755771637, accuracy: 0.900390625\n",
      "batch 160 loss: 0.236996129155159, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20701934397220612, accuracy: 0.921875\n",
      "training loss: 0.22791842594742776, accuracy: 0.916146979225056\n",
      "validation loss: 0.22593730688095093, accuracy: 0.9162953408380238\n",
      "batch 0 loss: 0.2544717490673065, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22454628348350525, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24352118372917175, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.248737633228302, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.2749464511871338, accuracy: 0.8935546875\n",
      "batch 100 loss: 0.2216302752494812, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.21712462604045868, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24939367175102234, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23654398322105408, accuracy: 0.9130859375\n",
      "batch 180 loss: 0.20741310715675354, accuracy: 0.921875\n",
      "training loss: 0.22786482483148573, accuracy: 0.9161518620375559\n",
      "validation loss: 0.22597916424274445, accuracy: 0.9162367104440275\n",
      "batch 0 loss: 0.25467589497566223, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22433337569236755, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.2433677762746811, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.248902827501297, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.2748352587223053, accuracy: 0.8935546875\n",
      "batch 100 loss: 0.22156299650669098, accuracy: 0.9140625\n",
      "batch 120 loss: 0.21680966019630432, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.2490285485982895, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23640359938144684, accuracy: 0.9130859375\n",
      "batch 180 loss: 0.2072543054819107, accuracy: 0.921875\n",
      "training loss: 0.2278067534416914, accuracy: 0.916142096412556\n",
      "validation loss: 0.22587011754512787, accuracy: 0.9161976235146967\n",
      "batch 0 loss: 0.254433274269104, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22451068460941315, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24353253841400146, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24891629815101624, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.2748149037361145, accuracy: 0.8935546875\n",
      "batch 100 loss: 0.22152650356292725, accuracy: 0.9130859375\n",
      "batch 120 loss: 0.21696944534778595, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24896705150604248, accuracy: 0.900390625\n",
      "batch 160 loss: 0.2364690899848938, accuracy: 0.9130859375\n",
      "batch 180 loss: 0.20717912912368774, accuracy: 0.921875\n",
      "training loss: 0.2277927953004837, accuracy: 0.916176276100056\n",
      "validation loss: 0.22573673725128174, accuracy: 0.9162367104440275\n",
      "batch 0 loss: 0.25428420305252075, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22442500293254852, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.2438298761844635, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24904310703277588, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.2748551070690155, accuracy: 0.8935546875\n",
      "batch 100 loss: 0.2213813066482544, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21696940064430237, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24899403750896454, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23652687668800354, accuracy: 0.912109375\n",
      "batch 180 loss: 0.2069215476512909, accuracy: 0.921875\n",
      "training loss: 0.2277347294986248, accuracy: 0.916146979225056\n",
      "validation loss: 0.22580312192440033, accuracy: 0.9162757973733584\n",
      "batch 0 loss: 0.25444257259368896, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.2240906059741974, accuracy: 0.919921875\n",
      "batch 40 loss: 0.24314138293266296, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24894672632217407, accuracy: 0.912109375\n",
      "batch 80 loss: 0.2747798562049866, accuracy: 0.8935546875\n",
      "batch 100 loss: 0.2213379293680191, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21685734391212463, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24857184290885925, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23630023002624512, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20683413743972778, accuracy: 0.921875\n",
      "training loss: 0.2276620861887932, accuracy: 0.916220221412556\n",
      "validation loss: 0.22583678364753723, accuracy: 0.9162171669793621\n",
      "batch 0 loss: 0.25457602739334106, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22411611676216125, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24329917132854462, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24918651580810547, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.27463334798812866, accuracy: 0.8935546875\n",
      "batch 100 loss: 0.22128044068813324, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21674716472625732, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24875037372112274, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23628318309783936, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20717665553092957, accuracy: 0.921875\n",
      "training loss: 0.22765533141791822, accuracy: 0.916142096412556\n",
      "validation loss: 0.22566264867782593, accuracy: 0.9163735146966854\n",
      "batch 0 loss: 0.2543112337589264, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22402365505695343, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.2431444674730301, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24912908673286438, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.27459263801574707, accuracy: 0.8935546875\n",
      "batch 100 loss: 0.22140547633171082, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21666516363620758, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24887196719646454, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23627230525016785, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20747220516204834, accuracy: 0.921875\n",
      "training loss: 0.22760883539915086, accuracy: 0.916181158912556\n",
      "validation loss: 0.22584591805934906, accuracy: 0.9162757973733584\n",
      "batch 0 loss: 0.2546555995941162, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22377684712409973, accuracy: 0.919921875\n",
      "batch 40 loss: 0.24246175587177277, accuracy: 0.91015625\n",
      "batch 60 loss: 0.24896381795406342, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.27453354001045227, accuracy: 0.8955078125\n",
      "batch 100 loss: 0.2213018536567688, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.216810405254364, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24893435835838318, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23616492748260498, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20699366927146912, accuracy: 0.921875\n",
      "training loss: 0.22756403349339963, accuracy: 0.916225104225056\n",
      "validation loss: 0.22569917142391205, accuracy: 0.9163344277673546\n",
      "batch 0 loss: 0.25445735454559326, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.2234629988670349, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24305471777915955, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24904704093933105, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.2744240462779999, accuracy: 0.8955078125\n",
      "batch 100 loss: 0.22125336527824402, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21680304408073425, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24863994121551514, accuracy: 0.900390625\n",
      "batch 160 loss: 0.236139178276062, accuracy: 0.912109375\n",
      "batch 180 loss: 0.2076198011636734, accuracy: 0.921875\n",
      "training loss: 0.2275180424749851, accuracy: 0.916186041725056\n",
      "validation loss: 0.2256559431552887, accuracy: 0.9163344277673546\n",
      "batch 0 loss: 0.2544509172439575, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.2234748899936676, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24289068579673767, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.2490190714597702, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.2744305729866028, accuracy: 0.8935546875\n",
      "batch 100 loss: 0.22132804989814758, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21676889061927795, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24864467978477478, accuracy: 0.900390625\n",
      "batch 160 loss: 0.2359924465417862, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20763573050498962, accuracy: 0.921875\n",
      "training loss: 0.22744568616151808, accuracy: 0.916166510475056\n",
      "validation loss: 0.22576117515563965, accuracy: 0.9163735146966854\n",
      "batch 0 loss: 0.2548137903213501, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22360897064208984, accuracy: 0.919921875\n",
      "batch 40 loss: 0.24295133352279663, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24902570247650146, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.2744789719581604, accuracy: 0.8935546875\n",
      "batch 100 loss: 0.2212037295103073, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21663898229599, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24804997444152832, accuracy: 0.900390625\n",
      "batch 160 loss: 0.2357647716999054, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20742854475975037, accuracy: 0.921875\n",
      "training loss: 0.22741172924637795, accuracy: 0.916137213600056\n",
      "validation loss: 0.22560857236385345, accuracy: 0.91635397123202\n",
      "batch 0 loss: 0.2544144093990326, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22320154309272766, accuracy: 0.919921875\n",
      "batch 40 loss: 0.2429359257221222, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24912816286087036, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.2744675278663635, accuracy: 0.8935546875\n",
      "batch 100 loss: 0.22126725316047668, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21667513251304626, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24836459755897522, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23581081628799438, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20736168324947357, accuracy: 0.921875\n",
      "training loss: 0.2273925992101431, accuracy: 0.9161518620375559\n",
      "validation loss: 0.22557058930397034, accuracy: 0.9163344277673546\n",
      "batch 0 loss: 0.2544122338294983, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.2235335409641266, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24299782514572144, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24907469749450684, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.27449649572372437, accuracy: 0.89453125\n",
      "batch 100 loss: 0.22136928141117096, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21681705117225647, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24828249216079712, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23580923676490784, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20705506205558777, accuracy: 0.921875\n",
      "training loss: 0.22737813748419286, accuracy: 0.916186041725056\n",
      "validation loss: 0.22559364140033722, accuracy: 0.9162757973733584\n",
      "batch 0 loss: 0.25440627336502075, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22331666946411133, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24284330010414124, accuracy: 0.91015625\n",
      "batch 60 loss: 0.24904900789260864, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.2744673490524292, accuracy: 0.8935546875\n",
      "batch 100 loss: 0.2212332785129547, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21669363975524902, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24857871234416962, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23627299070358276, accuracy: 0.912109375\n",
      "batch 180 loss: 0.2078799605369568, accuracy: 0.921875\n",
      "training loss: 0.2273324777930975, accuracy: 0.916200690162556\n",
      "validation loss: 0.2254762351512909, accuracy: 0.9163344277673546\n",
      "batch 0 loss: 0.254370778799057, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22316673398017883, accuracy: 0.919921875\n",
      "batch 40 loss: 0.24252855777740479, accuracy: 0.91015625\n",
      "batch 60 loss: 0.24889476597309113, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.27440503239631653, accuracy: 0.8935546875\n",
      "batch 100 loss: 0.22108399868011475, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21662668883800507, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24856367707252502, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23628313839435577, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20728091895580292, accuracy: 0.921875\n",
      "training loss: 0.22727952674031257, accuracy: 0.916176276100056\n",
      "validation loss: 0.22557297348976135, accuracy: 0.9163148843026891\n",
      "batch 0 loss: 0.25462090969085693, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.2227759212255478, accuracy: 0.919921875\n",
      "batch 40 loss: 0.24299103021621704, accuracy: 0.91015625\n",
      "batch 60 loss: 0.2489805817604065, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.2744465470314026, accuracy: 0.8935546875\n",
      "batch 100 loss: 0.22113558650016785, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21654057502746582, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24803480505943298, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23619097471237183, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20730620622634888, accuracy: 0.921875\n",
      "training loss: 0.22724431939423084, accuracy: 0.916210455787556\n",
      "validation loss: 0.22561898827552795, accuracy: 0.9162953408380238\n",
      "batch 0 loss: 0.2547512948513031, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22310498356819153, accuracy: 0.919921875\n",
      "batch 40 loss: 0.24278624355793, accuracy: 0.91015625\n",
      "batch 60 loss: 0.24865874648094177, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2744072675704956, accuracy: 0.8935546875\n",
      "batch 100 loss: 0.22123120725154877, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21658122539520264, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.2486928105354309, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23625342547893524, accuracy: 0.912109375\n",
      "batch 180 loss: 0.2077847272157669, accuracy: 0.921875\n",
      "training loss: 0.22724564634263517, accuracy: 0.916181158912556\n",
      "validation loss: 0.2255389243364334, accuracy: 0.9162953408380238\n",
      "batch 0 loss: 0.2547796666622162, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.2229052037000656, accuracy: 0.919921875\n",
      "batch 40 loss: 0.24265819787979126, accuracy: 0.91015625\n",
      "batch 60 loss: 0.24868889153003693, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27437686920166016, accuracy: 0.8935546875\n",
      "batch 100 loss: 0.2209671437740326, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21659427881240845, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24802950024604797, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23616716265678406, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20716950297355652, accuracy: 0.921875\n",
      "training loss: 0.22719528049230575, accuracy: 0.916205572975056\n",
      "validation loss: 0.22561360895633698, accuracy: 0.9163344277673546\n",
      "batch 0 loss: 0.2547975182533264, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.2233625054359436, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24254107475280762, accuracy: 0.91015625\n",
      "batch 60 loss: 0.24873822927474976, accuracy: 0.91015625\n",
      "batch 80 loss: 0.274264931678772, accuracy: 0.8955078125\n",
      "batch 100 loss: 0.22101962566375732, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21643628180027008, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.2478596270084381, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23652154207229614, accuracy: 0.912109375\n",
      "batch 180 loss: 0.2072615623474121, accuracy: 0.921875\n",
      "training loss: 0.22717868067324162, accuracy: 0.916176276100056\n",
      "validation loss: 0.2255316525697708, accuracy: 0.91635397123202\n",
      "batch 0 loss: 0.2547208070755005, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.2232877016067505, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.2429714798927307, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24872145056724548, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.27424532175064087, accuracy: 0.8955078125\n",
      "batch 100 loss: 0.22091028094291687, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21652284264564514, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.2480550855398178, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23616555333137512, accuracy: 0.912109375\n",
      "batch 180 loss: 0.2070149928331375, accuracy: 0.9228515625\n",
      "training loss: 0.22712503515183927, accuracy: 0.916186041725056\n",
      "validation loss: 0.22548283636569977, accuracy: 0.91635397123202\n",
      "batch 0 loss: 0.2546137869358063, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22288775444030762, accuracy: 0.919921875\n",
      "batch 40 loss: 0.24278420209884644, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.2487289309501648, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2741197347640991, accuracy: 0.89453125\n",
      "batch 100 loss: 0.22102496027946472, accuracy: 0.916015625\n",
      "batch 120 loss: 0.21649418771266937, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24807098507881165, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23627369105815887, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20716512203216553, accuracy: 0.9228515625\n",
      "training loss: 0.22709266506135464, accuracy: 0.916190924537556\n",
      "validation loss: 0.2255438268184662, accuracy: 0.91635397123202\n",
      "batch 0 loss: 0.2548956871032715, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22306354343891144, accuracy: 0.919921875\n",
      "batch 40 loss: 0.2430005669593811, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.2484663873910904, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27404576539993286, accuracy: 0.8955078125\n",
      "batch 100 loss: 0.22099241614341736, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21641093492507935, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24774062633514404, accuracy: 0.900390625\n",
      "batch 160 loss: 0.2356351613998413, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20717164874076843, accuracy: 0.9228515625\n",
      "training loss: 0.22709590196609497, accuracy: 0.916181158912556\n",
      "validation loss: 0.22544130682945251, accuracy: 0.9163344277673546\n",
      "batch 0 loss: 0.2546806037425995, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22316348552703857, accuracy: 0.919921875\n",
      "batch 40 loss: 0.24266529083251953, accuracy: 0.91015625\n",
      "batch 60 loss: 0.24887119233608246, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.2741156220436096, accuracy: 0.8955078125\n",
      "batch 100 loss: 0.22096383571624756, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.2166016399860382, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.2479047030210495, accuracy: 0.900390625\n",
      "batch 160 loss: 0.2355041801929474, accuracy: 0.912109375\n",
      "batch 180 loss: 0.2073889523744583, accuracy: 0.921875\n",
      "training loss: 0.22704873383045196, accuracy: 0.916200690162556\n",
      "validation loss: 0.22543004155158997, accuracy: 0.9162757973733584\n",
      "batch 0 loss: 0.2546387016773224, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22300076484680176, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.2427414059638977, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24876005947589874, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.2741038203239441, accuracy: 0.8955078125\n",
      "batch 100 loss: 0.2210032343864441, accuracy: 0.916015625\n",
      "batch 120 loss: 0.21667054295539856, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24746562540531158, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23529908061027527, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20698559284210205, accuracy: 0.9228515625\n",
      "training loss: 0.22700101293623448, accuracy: 0.916195807350056\n",
      "validation loss: 0.22536174952983856, accuracy: 0.9163148843026891\n",
      "batch 0 loss: 0.2546589970588684, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22310754656791687, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24255995452404022, accuracy: 0.91015625\n",
      "batch 60 loss: 0.24883607029914856, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27408644556999207, accuracy: 0.89453125\n",
      "batch 100 loss: 0.22104308009147644, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.2165725827217102, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24740464985370636, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23566997051239014, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20693379640579224, accuracy: 0.9228515625\n",
      "training loss: 0.2269491808116436, accuracy: 0.916176276100056\n",
      "validation loss: 0.22535774111747742, accuracy: 0.9163344277673546\n",
      "batch 0 loss: 0.2546479105949402, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.2230606973171234, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.2428434193134308, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24885906279087067, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27406439185142517, accuracy: 0.8955078125\n",
      "batch 100 loss: 0.2210608869791031, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.2165762186050415, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24732528626918793, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23557770252227783, accuracy: 0.912109375\n",
      "batch 180 loss: 0.2072477787733078, accuracy: 0.9228515625\n",
      "training loss: 0.22692878246307374, accuracy: 0.916190924537556\n",
      "validation loss: 0.22530050575733185, accuracy: 0.9163735146966854\n",
      "batch 0 loss: 0.25454387068748474, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22289910912513733, accuracy: 0.919921875\n",
      "batch 40 loss: 0.242925226688385, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24865663051605225, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27407628297805786, accuracy: 0.8955078125\n",
      "batch 100 loss: 0.2210218608379364, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21639209985733032, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24731649458408356, accuracy: 0.900390625\n",
      "batch 160 loss: 0.2355555295944214, accuracy: 0.912109375\n",
      "batch 180 loss: 0.207524836063385, accuracy: 0.9228515625\n",
      "training loss: 0.22692882247269153, accuracy: 0.916195807350056\n",
      "validation loss: 0.225280299782753, accuracy: 0.9163930581613509\n",
      "batch 0 loss: 0.2544674575328827, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22312131524085999, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24284148216247559, accuracy: 0.91015625\n",
      "batch 60 loss: 0.2484247237443924, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2740347385406494, accuracy: 0.8955078125\n",
      "batch 100 loss: 0.22099408507347107, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.2163023054599762, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24760600924491882, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23634831607341766, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20730414986610413, accuracy: 0.921875\n",
      "training loss: 0.22690298952162266, accuracy: 0.916132330787556\n",
      "validation loss: 0.22522254288196564, accuracy: 0.91635397123202\n",
      "batch 0 loss: 0.25421643257141113, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22332756221294403, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24265600740909576, accuracy: 0.91015625\n",
      "batch 60 loss: 0.24852818250656128, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27399545907974243, accuracy: 0.8955078125\n",
      "batch 100 loss: 0.22103732824325562, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21643070876598358, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24667277932167053, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23600494861602783, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20732298493385315, accuracy: 0.9228515625\n",
      "training loss: 0.22684443555772305, accuracy: 0.9161616276625559\n",
      "validation loss: 0.22515353560447693, accuracy: 0.9163344277673546\n",
      "batch 0 loss: 0.2541704773902893, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22315458953380585, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24278020858764648, accuracy: 0.91015625\n",
      "batch 60 loss: 0.24893435835838318, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27404850721359253, accuracy: 0.8955078125\n",
      "batch 100 loss: 0.22105304896831512, accuracy: 0.9140625\n",
      "batch 120 loss: 0.21629825234413147, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.2469611018896103, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23609651625156403, accuracy: 0.912109375\n",
      "batch 180 loss: 0.2071988582611084, accuracy: 0.9228515625\n",
      "training loss: 0.22682256653904914, accuracy: 0.9161616276625559\n",
      "validation loss: 0.2251383364200592, accuracy: 0.9163930581613509\n",
      "batch 0 loss: 0.25415217876434326, accuracy: 0.9140625\n",
      "batch 20 loss: 0.2231261134147644, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.2426210641860962, accuracy: 0.91015625\n",
      "batch 60 loss: 0.24856144189834595, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2739120423793793, accuracy: 0.8955078125\n",
      "batch 100 loss: 0.22094479203224182, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21645104885101318, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24668125808238983, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23539353907108307, accuracy: 0.912109375\n",
      "batch 180 loss: 0.2071889042854309, accuracy: 0.9228515625\n",
      "training loss: 0.2268025003373623, accuracy: 0.916171393287556\n",
      "validation loss: 0.22507384419441223, accuracy: 0.9163930581613509\n",
      "batch 0 loss: 0.25412559509277344, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22296983003616333, accuracy: 0.919921875\n",
      "batch 40 loss: 0.24276134371757507, accuracy: 0.91015625\n",
      "batch 60 loss: 0.24880492687225342, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2739218771457672, accuracy: 0.8955078125\n",
      "batch 100 loss: 0.22099527716636658, accuracy: 0.9140625\n",
      "batch 120 loss: 0.2163555771112442, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24696946144104004, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23563161492347717, accuracy: 0.912109375\n",
      "batch 180 loss: 0.2071153223514557, accuracy: 0.921875\n",
      "training loss: 0.22675637915730476, accuracy: 0.916146979225056\n",
      "validation loss: 0.22511862218379974, accuracy: 0.9164516885553471\n",
      "batch 0 loss: 0.2541002035140991, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22325021028518677, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24281008541584015, accuracy: 0.91015625\n",
      "batch 60 loss: 0.24874716997146606, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2738969922065735, accuracy: 0.8955078125\n",
      "batch 100 loss: 0.22098343074321747, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21623189747333527, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.2470964789390564, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.2356688380241394, accuracy: 0.912109375\n",
      "batch 180 loss: 0.2071818709373474, accuracy: 0.9228515625\n",
      "training loss: 0.2267341088503599, accuracy: 0.916166510475056\n",
      "validation loss: 0.22518837451934814, accuracy: 0.9164126016260162\n",
      "batch 0 loss: 0.25410836935043335, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22342240810394287, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24288643896579742, accuracy: 0.9111328125\n",
      "batch 60 loss: 0.24855682253837585, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2738055884838104, accuracy: 0.8955078125\n",
      "batch 100 loss: 0.22095075249671936, accuracy: 0.916015625\n",
      "batch 120 loss: 0.2163337916135788, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24652314186096191, accuracy: 0.900390625\n",
      "batch 160 loss: 0.2356196492910385, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20686990022659302, accuracy: 0.9228515625\n",
      "training loss: 0.22671663358807564, accuracy: 0.916146979225056\n",
      "validation loss: 0.22515524923801422, accuracy: 0.9164516885553471\n",
      "batch 0 loss: 0.25417762994766235, accuracy: 0.9140625\n",
      "batch 20 loss: 0.2233927696943283, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24244080483913422, accuracy: 0.91015625\n",
      "batch 60 loss: 0.2484818398952484, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27389615774154663, accuracy: 0.8955078125\n",
      "batch 100 loss: 0.2209273874759674, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.216234028339386, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.2462967336177826, accuracy: 0.900390625\n",
      "batch 160 loss: 0.2353910207748413, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20729973912239075, accuracy: 0.9208984375\n",
      "training loss: 0.2266823337227106, accuracy: 0.916113522106222\n",
      "validation loss: 0.2250092476606369, accuracy: 0.9163735146966854\n",
      "batch 0 loss: 0.25401678681373596, accuracy: 0.9140625\n",
      "batch 20 loss: 0.2235848307609558, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24278536438941956, accuracy: 0.91015625\n",
      "batch 60 loss: 0.24851959943771362, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27386558055877686, accuracy: 0.8955078125\n",
      "batch 100 loss: 0.22094875574111938, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.2161272168159485, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.2466958612203598, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23519104719161987, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20816195011138916, accuracy: 0.9208984375\n",
      "training loss: 0.22665811970829963, accuracy: 0.916127447975056\n",
      "validation loss: 0.22497683763504028, accuracy: 0.9163930581613509\n",
      "batch 0 loss: 0.25385352969169617, accuracy: 0.9140625\n",
      "batch 20 loss: 0.2232421636581421, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24243353307247162, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24863123893737793, accuracy: 0.9111328125\n",
      "batch 80 loss: 0.2738077640533447, accuracy: 0.8955078125\n",
      "batch 100 loss: 0.22101323306560516, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21610039472579956, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24659663438796997, accuracy: 0.900390625\n",
      "batch 160 loss: 0.2354532778263092, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20743601024150848, accuracy: 0.9208984375\n",
      "training loss: 0.2266406998038292, accuracy: 0.9161616276625559\n",
      "validation loss: 0.22506456077098846, accuracy: 0.9164126016260162\n",
      "batch 0 loss: 0.25414952635765076, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22368638217449188, accuracy: 0.919921875\n",
      "batch 40 loss: 0.24234884977340698, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24857595562934875, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2738012671470642, accuracy: 0.8955078125\n",
      "batch 100 loss: 0.22095605731010437, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.2161504626274109, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.2462122142314911, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23533368110656738, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20735670626163483, accuracy: 0.9208984375\n",
      "training loss: 0.22661983087658882, accuracy: 0.916176276100056\n",
      "validation loss: 0.22494496405124664, accuracy: 0.9162757973733584\n",
      "batch 0 loss: 0.2540311813354492, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22348056733608246, accuracy: 0.919921875\n",
      "batch 40 loss: 0.24225519597530365, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24843455851078033, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2737324833869934, accuracy: 0.8955078125\n",
      "batch 100 loss: 0.22090810537338257, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21599341928958893, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24634501338005066, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23544451594352722, accuracy: 0.912109375\n",
      "batch 180 loss: 0.2074260115623474, accuracy: 0.9208984375\n",
      "training loss: 0.22659638784825803, accuracy: 0.916132330787556\n",
      "validation loss: 0.22498825192451477, accuracy: 0.9164712320200125\n",
      "batch 0 loss: 0.2539752125740051, accuracy: 0.9140625\n",
      "batch 20 loss: 0.2233651727437973, accuracy: 0.919921875\n",
      "batch 40 loss: 0.24249698221683502, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24858638644218445, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2737285792827606, accuracy: 0.8955078125\n",
      "batch 100 loss: 0.22091639041900635, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.2161354124546051, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24743929505348206, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.23536571860313416, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20741663873195648, accuracy: 0.9208984375\n",
      "training loss: 0.2265518756210804, accuracy: 0.916191647106222\n",
      "validation loss: 0.22494882345199585, accuracy: 0.91635397123202\n",
      "batch 0 loss: 0.25400304794311523, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22344031929969788, accuracy: 0.919921875\n",
      "batch 40 loss: 0.24250352382659912, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24846318364143372, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2736842632293701, accuracy: 0.8955078125\n",
      "batch 100 loss: 0.22095409035682678, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.2160402089357376, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24615493416786194, accuracy: 0.900390625\n",
      "batch 160 loss: 0.2353413701057434, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20742252469062805, accuracy: 0.9208984375\n",
      "training loss: 0.2265296345204115, accuracy: 0.916166510475056\n",
      "validation loss: 0.22494885325431824, accuracy: 0.9162953408380238\n",
      "batch 0 loss: 0.25401559472084045, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22308124601840973, accuracy: 0.919921875\n",
      "batch 40 loss: 0.24237406253814697, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24818652868270874, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2736867368221283, accuracy: 0.8955078125\n",
      "batch 100 loss: 0.2209494709968567, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21612709760665894, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.2461073398590088, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.23544642329216003, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20704928040504456, accuracy: 0.9208984375\n",
      "training loss: 0.22650515511631966, accuracy: 0.916171393287556\n",
      "validation loss: 0.22498199343681335, accuracy: 0.9162953408380238\n",
      "batch 0 loss: 0.2540974020957947, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22290940582752228, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24252557754516602, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.2482599914073944, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27357813715934753, accuracy: 0.8955078125\n",
      "batch 100 loss: 0.22087882459163666, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.2161751091480255, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24557428061962128, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.23523679375648499, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20734421908855438, accuracy: 0.9208984375\n",
      "training loss: 0.2264645377546549, accuracy: 0.9161518620375559\n",
      "validation loss: 0.22493164241313934, accuracy: 0.9163735146966854\n",
      "batch 0 loss: 0.25386419892311096, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22300243377685547, accuracy: 0.9189453125\n",
      "batch 40 loss: 0.24250391125679016, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.2483421266078949, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2736683487892151, accuracy: 0.8955078125\n",
      "batch 100 loss: 0.22093427181243896, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.2161743938922882, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.2458304911851883, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.23548781871795654, accuracy: 0.912109375\n",
      "batch 180 loss: 0.2075079083442688, accuracy: 0.9208984375\n",
      "training loss: 0.22646542243659495, accuracy: 0.916186764293722\n",
      "validation loss: 0.2249017208814621, accuracy: 0.91635397123202\n",
      "batch 0 loss: 0.2538222670555115, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22335124015808105, accuracy: 0.919921875\n",
      "batch 40 loss: 0.24217462539672852, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.2483251839876175, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2735569477081299, accuracy: 0.896484375\n",
      "batch 100 loss: 0.22094041109085083, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21601781249046326, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.2468770146369934, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.23532310128211975, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20699390769004822, accuracy: 0.9208984375\n",
      "training loss: 0.22641599148511887, accuracy: 0.916166510475056\n",
      "validation loss: 0.225010946393013, accuracy: 0.9163930581613509\n",
      "batch 0 loss: 0.25408321619033813, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22306792438030243, accuracy: 0.919921875\n",
      "batch 40 loss: 0.24227634072303772, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24807417392730713, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2736007571220398, accuracy: 0.896484375\n",
      "batch 100 loss: 0.22099407017230988, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21617048978805542, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.2466106116771698, accuracy: 0.900390625\n",
      "batch 160 loss: 0.2352074384689331, accuracy: 0.912109375\n",
      "batch 180 loss: 0.2071603685617447, accuracy: 0.9208984375\n",
      "training loss: 0.22639377124607563, accuracy: 0.916172115856222\n",
      "validation loss: 0.22485485672950745, accuracy: 0.9163344277673546\n",
      "batch 0 loss: 0.2538650333881378, accuracy: 0.9140625\n",
      "batch 20 loss: 0.2233341634273529, accuracy: 0.919921875\n",
      "batch 40 loss: 0.242320716381073, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24813289940357208, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27354639768600464, accuracy: 0.8955078125\n",
      "batch 100 loss: 0.2209641933441162, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21600303053855896, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24544700980186462, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.23545458912849426, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20715418457984924, accuracy: 0.9208984375\n",
      "training loss: 0.22639238372445106, accuracy: 0.916215338600056\n",
      "validation loss: 0.22496195137500763, accuracy: 0.9163735146966854\n",
      "batch 0 loss: 0.25407305359840393, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22287726402282715, accuracy: 0.919921875\n",
      "batch 40 loss: 0.2423168420791626, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24836082756519318, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27343112230300903, accuracy: 0.896484375\n",
      "batch 100 loss: 0.2209409773349762, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.2160942256450653, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.2463500201702118, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.235164076089859, accuracy: 0.912109375\n",
      "batch 180 loss: 0.2068713903427124, accuracy: 0.9208984375\n",
      "training loss: 0.2263376746326685, accuracy: 0.916190924537556\n",
      "validation loss: 0.22492770850658417, accuracy: 0.9163344277673546\n",
      "batch 0 loss: 0.25416189432144165, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22300609946250916, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.2422264814376831, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24847696721553802, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2734851837158203, accuracy: 0.896484375\n",
      "batch 100 loss: 0.22096481919288635, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21606025099754333, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24565112590789795, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.2352418303489685, accuracy: 0.912109375\n",
      "batch 180 loss: 0.2069595754146576, accuracy: 0.9208984375\n",
      "training loss: 0.22631235785782336, accuracy: 0.916225104225056\n",
      "validation loss: 0.2248913049697876, accuracy: 0.9163735146966854\n",
      "batch 0 loss: 0.2539311349391937, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22296439111232758, accuracy: 0.919921875\n",
      "batch 40 loss: 0.24184128642082214, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.2482735514640808, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2733756899833679, accuracy: 0.896484375\n",
      "batch 100 loss: 0.22101858258247375, accuracy: 0.916015625\n",
      "batch 120 loss: 0.21597981452941895, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.2451361119747162, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.2355194240808487, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20699381828308105, accuracy: 0.9208984375\n",
      "training loss: 0.2262817507982254, accuracy: 0.916249518287556\n",
      "validation loss: 0.22487664222717285, accuracy: 0.9162757973733584\n",
      "batch 0 loss: 0.25380268692970276, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22270473837852478, accuracy: 0.919921875\n",
      "batch 40 loss: 0.24190261960029602, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.2482861578464508, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27335020899772644, accuracy: 0.896484375\n",
      "batch 100 loss: 0.2209969162940979, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21585574746131897, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24569737911224365, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.23511503636837006, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20689885318279266, accuracy: 0.921875\n",
      "training loss: 0.2262790412455797, accuracy: 0.916244635475056\n",
      "validation loss: 0.2248932421207428, accuracy: 0.9162757973733584\n",
      "batch 0 loss: 0.2539442777633667, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22274461388587952, accuracy: 0.919921875\n",
      "batch 40 loss: 0.24207806587219238, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24805501103401184, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27336937189102173, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.22104421257972717, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.2158418595790863, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.2453819215297699, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.2350870817899704, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20691783726215363, accuracy: 0.9208984375\n",
      "training loss: 0.22623068049550057, accuracy: 0.916244635475056\n",
      "validation loss: 0.22493202984333038, accuracy: 0.9163148843026891\n",
      "batch 0 loss: 0.25397515296936035, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22253987193107605, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24181726574897766, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24822679162025452, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2733790874481201, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.2211313396692276, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21588025987148285, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24563732743263245, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.23525899648666382, accuracy: 0.9111328125\n",
      "batch 180 loss: 0.20725031197071075, accuracy: 0.919921875\n",
      "training loss: 0.22623841643333434, accuracy: 0.916264166725056\n",
      "validation loss: 0.22486090660095215, accuracy: 0.9162953408380238\n",
      "batch 0 loss: 0.2540583610534668, accuracy: 0.9140625\n",
      "batch 20 loss: 0.2227521687746048, accuracy: 0.919921875\n",
      "batch 40 loss: 0.24181167781352997, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.247988760471344, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27336999773979187, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.22100481390953064, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.2157677561044693, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24589158594608307, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.23497459292411804, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20751702785491943, accuracy: 0.9189453125\n",
      "training loss: 0.2262004702538252, accuracy: 0.916229987037556\n",
      "validation loss: 0.22487013041973114, accuracy: 0.9162562539086929\n",
      "batch 0 loss: 0.25416100025177, accuracy: 0.9150390625\n",
      "batch 20 loss: 0.2229573130607605, accuracy: 0.919921875\n",
      "batch 40 loss: 0.2417771816253662, accuracy: 0.91015625\n",
      "batch 60 loss: 0.24792703986167908, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2734469771385193, accuracy: 0.896484375\n",
      "batch 100 loss: 0.22107326984405518, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21566933393478394, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24534481763839722, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.2351108342409134, accuracy: 0.912109375\n",
      "batch 180 loss: 0.2074040174484253, accuracy: 0.9189453125\n",
      "training loss: 0.22617776669561862, accuracy: 0.916220221412556\n",
      "validation loss: 0.2248699814081192, accuracy: 0.9163344277673546\n",
      "batch 0 loss: 0.25392434000968933, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22250694036483765, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24163198471069336, accuracy: 0.91015625\n",
      "batch 60 loss: 0.2480514943599701, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27348366379737854, accuracy: 0.896484375\n",
      "batch 100 loss: 0.22096684575080872, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21579334139823914, accuracy: 0.921875\n",
      "batch 140 loss: 0.24506154656410217, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.23530301451683044, accuracy: 0.912109375\n",
      "batch 180 loss: 0.2073812633752823, accuracy: 0.919921875\n",
      "training loss: 0.2261598014086485, accuracy: 0.916259283912556\n",
      "validation loss: 0.22476007044315338, accuracy: 0.9162367104440275\n",
      "batch 0 loss: 0.2537764012813568, accuracy: 0.9150390625\n",
      "batch 20 loss: 0.22215953469276428, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24141138792037964, accuracy: 0.91015625\n",
      "batch 60 loss: 0.24817824363708496, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27340835332870483, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.2210780680179596, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21579892933368683, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.2451326549053192, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.235122412443161, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20732492208480835, accuracy: 0.9189453125\n",
      "training loss: 0.22614300422370434, accuracy: 0.9162836979750559\n",
      "validation loss: 0.2246554046869278, accuracy: 0.9161780800500313\n",
      "batch 0 loss: 0.2536444365978241, accuracy: 0.9150390625\n",
      "batch 20 loss: 0.22252994775772095, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24163024127483368, accuracy: 0.91015625\n",
      "batch 60 loss: 0.2481023073196411, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27341464161872864, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.22100478410720825, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21589243412017822, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.2451222687959671, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.23518307507038116, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20747852325439453, accuracy: 0.919921875\n",
      "training loss: 0.2261143612116575, accuracy: 0.9162739323500559\n",
      "validation loss: 0.2247321754693985, accuracy: 0.9160999061913696\n",
      "batch 0 loss: 0.25385597348213196, accuracy: 0.9150390625\n",
      "batch 20 loss: 0.22242793440818787, accuracy: 0.919921875\n",
      "batch 40 loss: 0.24170896410942078, accuracy: 0.91015625\n",
      "batch 60 loss: 0.24813120067119598, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2734604477882385, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.22094666957855225, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21579775214195251, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24481579661369324, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.23498070240020752, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20757459104061127, accuracy: 0.9189453125\n",
      "training loss: 0.22610674612224102, accuracy: 0.916234869850056\n",
      "validation loss: 0.22469814121723175, accuracy: 0.9161585365853658\n",
      "batch 0 loss: 0.25371405482292175, accuracy: 0.9150390625\n",
      "batch 20 loss: 0.22256827354431152, accuracy: 0.919921875\n",
      "batch 40 loss: 0.24169513583183289, accuracy: 0.91015625\n",
      "batch 60 loss: 0.24831156432628632, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2733023166656494, accuracy: 0.896484375\n",
      "batch 100 loss: 0.22099722921848297, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.215619757771492, accuracy: 0.921875\n",
      "batch 140 loss: 0.24478663504123688, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.23505590856075287, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20731107890605927, accuracy: 0.9189453125\n",
      "training loss: 0.22608663626015185, accuracy: 0.916288580787556\n",
      "validation loss: 0.2247454822063446, accuracy: 0.9161780800500313\n",
      "batch 0 loss: 0.25372064113616943, accuracy: 0.9150390625\n",
      "batch 20 loss: 0.22241324186325073, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24177375435829163, accuracy: 0.91015625\n",
      "batch 60 loss: 0.24794752895832062, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27332067489624023, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.2210332155227661, accuracy: 0.916015625\n",
      "batch 120 loss: 0.21565085649490356, accuracy: 0.921875\n",
      "batch 140 loss: 0.24477526545524597, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.23510226607322693, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20707377791404724, accuracy: 0.919921875\n",
      "training loss: 0.2260404122620821, accuracy: 0.916308112037556\n",
      "validation loss: 0.2247997522354126, accuracy: 0.9161585365853658\n",
      "batch 0 loss: 0.254050612449646, accuracy: 0.9150390625\n",
      "batch 20 loss: 0.22236919403076172, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24141255021095276, accuracy: 0.91015625\n",
      "batch 60 loss: 0.248008131980896, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2733848989009857, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.2209908813238144, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21565663814544678, accuracy: 0.921875\n",
      "batch 140 loss: 0.24484483897686005, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.23520198464393616, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20739874243736267, accuracy: 0.919921875\n",
      "training loss: 0.22604732424020768, accuracy: 0.916298346412556\n",
      "validation loss: 0.22468307614326477, accuracy: 0.9160999061913696\n",
      "batch 0 loss: 0.2537335157394409, accuracy: 0.9150390625\n",
      "batch 20 loss: 0.22240325808525085, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24134090542793274, accuracy: 0.91015625\n",
      "batch 60 loss: 0.24788862466812134, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27332252264022827, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.22108156979084015, accuracy: 0.916015625\n",
      "batch 120 loss: 0.2156275510787964, accuracy: 0.921875\n",
      "batch 140 loss: 0.24459022283554077, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.23514264822006226, accuracy: 0.9111328125\n",
      "batch 180 loss: 0.20772066712379456, accuracy: 0.9189453125\n",
      "training loss: 0.22602720580995084, accuracy: 0.916317877662556\n",
      "validation loss: 0.22475764155387878, accuracy: 0.9161389931207005\n",
      "batch 0 loss: 0.25394612550735474, accuracy: 0.9150390625\n",
      "batch 20 loss: 0.2222091257572174, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.2412315458059311, accuracy: 0.91015625\n",
      "batch 60 loss: 0.24809306859970093, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2732952833175659, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.2209669053554535, accuracy: 0.916015625\n",
      "batch 120 loss: 0.21555382013320923, accuracy: 0.921875\n",
      "batch 140 loss: 0.24473994970321655, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23489654064178467, accuracy: 0.9111328125\n",
      "batch 180 loss: 0.20755484700202942, accuracy: 0.9189453125\n",
      "training loss: 0.22600720167160035, accuracy: 0.916264166725056\n",
      "validation loss: 0.224722757935524, accuracy: 0.9160803627267042\n",
      "batch 0 loss: 0.2537479102611542, accuracy: 0.9150390625\n",
      "batch 20 loss: 0.22220230102539062, accuracy: 0.919921875\n",
      "batch 40 loss: 0.2413257658481598, accuracy: 0.91015625\n",
      "batch 60 loss: 0.24790532886981964, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27337443828582764, accuracy: 0.896484375\n",
      "batch 100 loss: 0.22101742029190063, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.2156212478876114, accuracy: 0.921875\n",
      "batch 140 loss: 0.24425067007541656, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23493364453315735, accuracy: 0.912109375\n",
      "batch 180 loss: 0.2068111002445221, accuracy: 0.9208984375\n",
      "training loss: 0.22597813181579113, accuracy: 0.916298346412556\n",
      "validation loss: 0.2247709035873413, accuracy: 0.916119449656035\n",
      "batch 0 loss: 0.25384649634361267, accuracy: 0.9140625\n",
      "batch 20 loss: 0.2222113460302353, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24129830300807953, accuracy: 0.91015625\n",
      "batch 60 loss: 0.24805957078933716, accuracy: 0.91015625\n",
      "batch 80 loss: 0.273343563079834, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.2209743708372116, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21559083461761475, accuracy: 0.921875\n",
      "batch 140 loss: 0.24453949928283691, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23502683639526367, accuracy: 0.9111328125\n",
      "batch 180 loss: 0.20705948770046234, accuracy: 0.9189453125\n",
      "training loss: 0.22597916714847088, accuracy: 0.916312994850056\n",
      "validation loss: 0.22465820610523224, accuracy: 0.9160412757973734\n",
      "batch 0 loss: 0.25366926193237305, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22214540839195251, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24128590524196625, accuracy: 0.91015625\n",
      "batch 60 loss: 0.2479369044303894, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27333879470825195, accuracy: 0.896484375\n",
      "batch 100 loss: 0.22097069025039673, accuracy: 0.916015625\n",
      "batch 120 loss: 0.21560005843639374, accuracy: 0.921875\n",
      "batch 140 loss: 0.24532775580883026, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23508909344673157, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20672474801540375, accuracy: 0.9208984375\n",
      "training loss: 0.2259531057626009, accuracy: 0.916317877662556\n",
      "validation loss: 0.22467727959156036, accuracy: 0.9160608192620388\n",
      "batch 0 loss: 0.25363925099372864, accuracy: 0.9140625\n",
      "batch 20 loss: 0.2221466451883316, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24116508662700653, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24791139364242554, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27332764863967896, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.2209797203540802, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21557484567165375, accuracy: 0.9208984375\n",
      "batch 140 loss: 0.24504327774047852, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.23515722155570984, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20726169645786285, accuracy: 0.9208984375\n",
      "training loss: 0.22593610651791096, accuracy: 0.9162836979750559\n",
      "validation loss: 0.22473260760307312, accuracy: 0.9162562539086929\n",
      "batch 0 loss: 0.2538435459136963, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22226041555404663, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24135297536849976, accuracy: 0.91015625\n",
      "batch 60 loss: 0.24805791676044464, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27333545684814453, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.22105297446250916, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21553322672843933, accuracy: 0.921875\n",
      "batch 140 loss: 0.24447749555110931, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23514670133590698, accuracy: 0.912109375\n",
      "batch 180 loss: 0.2074965238571167, accuracy: 0.919921875\n",
      "training loss: 0.2259095126390457, accuracy: 0.916312994850056\n",
      "validation loss: 0.22470884025096893, accuracy: 0.9161389931207005\n",
      "batch 0 loss: 0.2538450360298157, accuracy: 0.9140625\n",
      "batch 20 loss: 0.2220354676246643, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24129410088062286, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24775342643260956, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2732860743999481, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.2210870087146759, accuracy: 0.916015625\n",
      "batch 120 loss: 0.21563032269477844, accuracy: 0.921875\n",
      "batch 140 loss: 0.24470698833465576, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23514235019683838, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20699337124824524, accuracy: 0.9208984375\n",
      "training loss: 0.22588845193386078, accuracy: 0.916332526100056\n",
      "validation loss: 0.2246643304824829, accuracy: 0.9160021888680425\n",
      "batch 0 loss: 0.25395020842552185, accuracy: 0.9150390625\n",
      "batch 20 loss: 0.2221376597881317, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24106238782405853, accuracy: 0.91015625\n",
      "batch 60 loss: 0.2479751706123352, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27329370379447937, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.22104832530021667, accuracy: 0.916015625\n",
      "batch 120 loss: 0.21553540229797363, accuracy: 0.921875\n",
      "batch 140 loss: 0.24423852562904358, accuracy: 0.900390625\n",
      "batch 160 loss: 0.2352999746799469, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20717068016529083, accuracy: 0.9208984375\n",
      "training loss: 0.22586884886026382, accuracy: 0.916317877662556\n",
      "validation loss: 0.2246101051568985, accuracy: 0.9160021888680425\n",
      "batch 0 loss: 0.253704696893692, accuracy: 0.9140625\n",
      "batch 20 loss: 0.2219162881374359, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.2412700057029724, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.2478138953447342, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2733219265937805, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.22101661562919617, accuracy: 0.916015625\n",
      "batch 120 loss: 0.21552929282188416, accuracy: 0.921875\n",
      "batch 140 loss: 0.2445027232170105, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23562680184841156, accuracy: 0.91015625\n",
      "batch 180 loss: 0.2067827582359314, accuracy: 0.9208984375\n",
      "training loss: 0.22585544250905515, accuracy: 0.916303229225056\n",
      "validation loss: 0.22473815083503723, accuracy: 0.9160021888680425\n",
      "batch 0 loss: 0.25398796796798706, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22194761037826538, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24123705923557281, accuracy: 0.91015625\n",
      "batch 60 loss: 0.24786269664764404, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2730518877506256, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.22085031867027283, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21554605662822723, accuracy: 0.921875\n",
      "batch 140 loss: 0.2444194257259369, accuracy: 0.900390625\n",
      "batch 160 loss: 0.2352619767189026, accuracy: 0.9111328125\n",
      "batch 180 loss: 0.20706453919410706, accuracy: 0.9208984375\n",
      "training loss: 0.22583686366677283, accuracy: 0.9162788151625559\n",
      "validation loss: 0.22462330758571625, accuracy: 0.916119449656035\n",
      "batch 0 loss: 0.25382980704307556, accuracy: 0.9140625\n",
      "batch 20 loss: 0.2218320071697235, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24111589789390564, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24772107601165771, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2732502222061157, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.22109732031822205, accuracy: 0.916015625\n",
      "batch 120 loss: 0.21559296548366547, accuracy: 0.921875\n",
      "batch 140 loss: 0.24451598525047302, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23535031080245972, accuracy: 0.9111328125\n",
      "batch 180 loss: 0.20728705823421478, accuracy: 0.921875\n",
      "training loss: 0.22582427181303502, accuracy: 0.916347174537556\n",
      "validation loss: 0.22462260723114014, accuracy: 0.9160803627267042\n",
      "batch 0 loss: 0.25377312302589417, accuracy: 0.9150390625\n",
      "batch 20 loss: 0.22202229499816895, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24130527675151825, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24776604771614075, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27316606044769287, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.22088973224163055, accuracy: 0.916015625\n",
      "batch 120 loss: 0.21553649008274078, accuracy: 0.921875\n",
      "batch 140 loss: 0.2445744127035141, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23465672135353088, accuracy: 0.912109375\n",
      "batch 180 loss: 0.20722898840904236, accuracy: 0.921875\n",
      "training loss: 0.22579482957720756, accuracy: 0.916347174537556\n",
      "validation loss: 0.2246101051568985, accuracy: 0.9160803627267042\n",
      "batch 0 loss: 0.25369885563850403, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22185057401657104, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24112369120121002, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24755138158798218, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2731451094150543, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.22107839584350586, accuracy: 0.916015625\n",
      "batch 120 loss: 0.21546608209609985, accuracy: 0.921875\n",
      "batch 140 loss: 0.2441793978214264, accuracy: 0.900390625\n",
      "batch 160 loss: 0.235329732298851, accuracy: 0.91015625\n",
      "batch 180 loss: 0.20683011412620544, accuracy: 0.921875\n",
      "training loss: 0.2257782104611397, accuracy: 0.916308112037556\n",
      "validation loss: 0.22463344037532806, accuracy: 0.9160608192620388\n",
      "batch 0 loss: 0.25377848744392395, accuracy: 0.9150390625\n",
      "batch 20 loss: 0.22210733592510223, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24088698625564575, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24754545092582703, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2732433080673218, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.22104445099830627, accuracy: 0.916015625\n",
      "batch 120 loss: 0.21541722118854523, accuracy: 0.921875\n",
      "batch 140 loss: 0.24416710436344147, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23558196425437927, accuracy: 0.91015625\n",
      "batch 180 loss: 0.20702819526195526, accuracy: 0.921875\n",
      "training loss: 0.22576731085777282, accuracy: 0.916332526100056\n",
      "validation loss: 0.2246718853712082, accuracy: 0.9160217323327079\n",
      "batch 0 loss: 0.2539938688278198, accuracy: 0.9150390625\n",
      "batch 20 loss: 0.22179806232452393, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24090752005577087, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24733223021030426, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27321183681488037, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.2209487408399582, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21543443202972412, accuracy: 0.921875\n",
      "batch 140 loss: 0.24401837587356567, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.23528869450092316, accuracy: 0.91015625\n",
      "batch 180 loss: 0.207780122756958, accuracy: 0.921875\n",
      "training loss: 0.22575442038476468, accuracy: 0.916308112037556\n",
      "validation loss: 0.22462409734725952, accuracy: 0.9160999061913696\n",
      "batch 0 loss: 0.25382429361343384, accuracy: 0.9150390625\n",
      "batch 20 loss: 0.22181111574172974, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24117350578308105, accuracy: 0.91015625\n",
      "batch 60 loss: 0.24750448763370514, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2733101546764374, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.2209813892841339, accuracy: 0.916015625\n",
      "batch 120 loss: 0.21546363830566406, accuracy: 0.921875\n",
      "batch 140 loss: 0.24422794580459595, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23464073240756989, accuracy: 0.912109375\n",
      "batch 180 loss: 0.2071002870798111, accuracy: 0.921875\n",
      "training loss: 0.2257189942151308, accuracy: 0.916381354225056\n",
      "validation loss: 0.22464829683303833, accuracy: 0.9160803627267042\n",
      "batch 0 loss: 0.25380027294158936, accuracy: 0.9150390625\n",
      "batch 20 loss: 0.2219875305891037, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.2410716563463211, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24742865562438965, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2731211185455322, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.22098173201084137, accuracy: 0.916015625\n",
      "batch 120 loss: 0.21545317769050598, accuracy: 0.921875\n",
      "batch 140 loss: 0.2438875287771225, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.23525354266166687, accuracy: 0.9111328125\n",
      "batch 180 loss: 0.20702296495437622, accuracy: 0.921875\n",
      "training loss: 0.22569717802107334, accuracy: 0.916366705787556\n",
      "validation loss: 0.22462373971939087, accuracy: 0.9160803627267042\n",
      "batch 0 loss: 0.2537689208984375, accuracy: 0.9150390625\n",
      "batch 20 loss: 0.2217453122138977, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24057462811470032, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24754735827445984, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2730709910392761, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.22100520133972168, accuracy: 0.916015625\n",
      "batch 120 loss: 0.2156042903661728, accuracy: 0.921875\n",
      "batch 140 loss: 0.24417883157730103, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23484939336776733, accuracy: 0.912109375\n",
      "batch 180 loss: 0.2074965238571167, accuracy: 0.921875\n",
      "training loss: 0.22563643112778664, accuracy: 0.916352057350056\n",
      "validation loss: 0.22455568611621857, accuracy: 0.9159826454033771\n",
      "batch 0 loss: 0.25369349122047424, accuracy: 0.9150390625\n",
      "batch 20 loss: 0.2217540740966797, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24085694551467896, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24754589796066284, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27312085032463074, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.2210535854101181, accuracy: 0.916015625\n",
      "batch 120 loss: 0.21543368697166443, accuracy: 0.921875\n",
      "batch 140 loss: 0.24383977055549622, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23513230681419373, accuracy: 0.91015625\n",
      "batch 180 loss: 0.20683562755584717, accuracy: 0.9228515625\n",
      "training loss: 0.22563882052898407, accuracy: 0.916376471412556\n",
      "validation loss: 0.22461570799350739, accuracy: 0.9160021888680425\n",
      "batch 0 loss: 0.25379329919815063, accuracy: 0.9150390625\n",
      "batch 20 loss: 0.2217048704624176, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.2409190684556961, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.247304767370224, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27321383357048035, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.22110897302627563, accuracy: 0.916015625\n",
      "batch 120 loss: 0.21537813544273376, accuracy: 0.921875\n",
      "batch 140 loss: 0.24368759989738464, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23518383502960205, accuracy: 0.91015625\n",
      "batch 180 loss: 0.20634764432907104, accuracy: 0.9228515625\n",
      "training loss: 0.22561470702290534, accuracy: 0.916371588600056\n",
      "validation loss: 0.22459492087364197, accuracy: 0.9161780800500313\n",
      "batch 0 loss: 0.2537817060947418, accuracy: 0.9150390625\n",
      "batch 20 loss: 0.22161202132701874, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24043957889080048, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24730044603347778, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2730935215950012, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.22103071212768555, accuracy: 0.916015625\n",
      "batch 120 loss: 0.2152664214372635, accuracy: 0.921875\n",
      "batch 140 loss: 0.2436652034521103, accuracy: 0.900390625\n",
      "batch 160 loss: 0.2356337457895279, accuracy: 0.9091796875\n",
      "batch 180 loss: 0.20694129168987274, accuracy: 0.921875\n",
      "training loss: 0.2256169793009758, accuracy: 0.916356940162556\n",
      "validation loss: 0.224570170044899, accuracy: 0.9160608192620388\n",
      "batch 0 loss: 0.25371816754341125, accuracy: 0.9150390625\n",
      "batch 20 loss: 0.2216944545507431, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24056443572044373, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24747765064239502, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27305397391319275, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.22092083096504211, accuracy: 0.916015625\n",
      "batch 120 loss: 0.21527493000030518, accuracy: 0.921875\n",
      "batch 140 loss: 0.24303674697875977, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23524826765060425, accuracy: 0.91015625\n",
      "batch 180 loss: 0.20652073621749878, accuracy: 0.9228515625\n",
      "training loss: 0.2256058467179537, accuracy: 0.916371588600056\n",
      "validation loss: 0.2245701253414154, accuracy: 0.9160412757973734\n",
      "batch 0 loss: 0.25394079089164734, accuracy: 0.9150390625\n",
      "batch 20 loss: 0.2215975821018219, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24050675332546234, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24753224849700928, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2729693055152893, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.2210359275341034, accuracy: 0.916015625\n",
      "batch 120 loss: 0.21545696258544922, accuracy: 0.921875\n",
      "batch 140 loss: 0.2430935502052307, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.23496003448963165, accuracy: 0.9111328125\n",
      "batch 180 loss: 0.20819713175296783, accuracy: 0.9189453125\n",
      "training loss: 0.22556108474731446, accuracy: 0.916352057350056\n",
      "validation loss: 0.22460846602916718, accuracy: 0.9160803627267042\n",
      "batch 0 loss: 0.25386637449264526, accuracy: 0.9150390625\n",
      "batch 20 loss: 0.22152449190616608, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24042224884033203, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24749626219272614, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2730322480201721, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.22092464566230774, accuracy: 0.916015625\n",
      "batch 120 loss: 0.21530646085739136, accuracy: 0.921875\n",
      "batch 140 loss: 0.24337053298950195, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.2347041666507721, accuracy: 0.9111328125\n",
      "batch 180 loss: 0.20714932680130005, accuracy: 0.9228515625\n",
      "training loss: 0.22556298613548278, accuracy: 0.9163960026625559\n",
      "validation loss: 0.22455748915672302, accuracy: 0.9160803627267042\n",
      "batch 0 loss: 0.2537415623664856, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22187241911888123, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24057655036449432, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24743202328681946, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27297455072402954, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.22104912996292114, accuracy: 0.916015625\n",
      "batch 120 loss: 0.2154240608215332, accuracy: 0.921875\n",
      "batch 140 loss: 0.24331951141357422, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23485279083251953, accuracy: 0.91015625\n",
      "batch 180 loss: 0.2067106068134308, accuracy: 0.9228515625\n",
      "training loss: 0.22553332455456257, accuracy: 0.916386237037556\n",
      "validation loss: 0.22462151944637299, accuracy: 0.9160999061913696\n",
      "batch 0 loss: 0.2538469433784485, accuracy: 0.9150390625\n",
      "batch 20 loss: 0.22158494591712952, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24055661261081696, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24742455780506134, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2729530334472656, accuracy: 0.896484375\n",
      "batch 100 loss: 0.2210073173046112, accuracy: 0.916015625\n",
      "batch 120 loss: 0.21533852815628052, accuracy: 0.921875\n",
      "batch 140 loss: 0.24344144761562347, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.23519647121429443, accuracy: 0.9091796875\n",
      "batch 180 loss: 0.20818279683589935, accuracy: 0.919921875\n",
      "training loss: 0.22551951564848424, accuracy: 0.916332526100056\n",
      "validation loss: 0.22459694743156433, accuracy: 0.9160608192620388\n",
      "batch 0 loss: 0.2537415325641632, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22176066040992737, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24054551124572754, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24736160039901733, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2729802131652832, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.22101646661758423, accuracy: 0.916015625\n",
      "batch 120 loss: 0.2154148817062378, accuracy: 0.921875\n",
      "batch 140 loss: 0.24351359903812408, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.23482182621955872, accuracy: 0.91015625\n",
      "batch 180 loss: 0.20685452222824097, accuracy: 0.9228515625\n",
      "training loss: 0.22550105534493922, accuracy: 0.9164008854750559\n",
      "validation loss: 0.22460214793682098, accuracy: 0.9160608192620388\n",
      "batch 0 loss: 0.25380900502204895, accuracy: 0.9150390625\n",
      "batch 20 loss: 0.22179099917411804, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24068695306777954, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.2472851276397705, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27300554513931274, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.2209373414516449, accuracy: 0.916015625\n",
      "batch 120 loss: 0.21521446108818054, accuracy: 0.921875\n",
      "batch 140 loss: 0.24374541640281677, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23473122715950012, accuracy: 0.91015625\n",
      "batch 180 loss: 0.20755639672279358, accuracy: 0.9208984375\n",
      "training loss: 0.22549372643232346, accuracy: 0.916356940162556\n",
      "validation loss: 0.22462651133537292, accuracy: 0.916119449656035\n",
      "batch 0 loss: 0.25399190187454224, accuracy: 0.9140625\n",
      "batch 20 loss: 0.2215677797794342, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24050717055797577, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24726776778697968, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27297133207321167, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.22105160355567932, accuracy: 0.916015625\n",
      "batch 120 loss: 0.21514369547367096, accuracy: 0.921875\n",
      "batch 140 loss: 0.24285680055618286, accuracy: 0.900390625\n",
      "batch 160 loss: 0.2353590428829193, accuracy: 0.9091796875\n",
      "batch 180 loss: 0.20750410854816437, accuracy: 0.923828125\n",
      "training loss: 0.22545190781354904, accuracy: 0.916356940162556\n",
      "validation loss: 0.22457455098628998, accuracy: 0.9161780800500313\n",
      "batch 0 loss: 0.25370803475379944, accuracy: 0.9150390625\n",
      "batch 20 loss: 0.22164911031723022, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.2404366135597229, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24750015139579773, accuracy: 0.91015625\n",
      "batch 80 loss: 0.2729930281639099, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.2210342437028885, accuracy: 0.916015625\n",
      "batch 120 loss: 0.21518899500370026, accuracy: 0.921875\n",
      "batch 140 loss: 0.24296525120735168, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.2350146770477295, accuracy: 0.91015625\n",
      "batch 180 loss: 0.20799991488456726, accuracy: 0.9208984375\n",
      "training loss: 0.22544289842247964, accuracy: 0.916356940162556\n",
      "validation loss: 0.22464315593242645, accuracy: 0.916119449656035\n",
      "batch 0 loss: 0.2538709044456482, accuracy: 0.9150390625\n",
      "batch 20 loss: 0.22159841656684875, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24065247178077698, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24755820631980896, accuracy: 0.91015625\n",
      "batch 80 loss: 0.27291759848594666, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.22106049954891205, accuracy: 0.916015625\n",
      "batch 120 loss: 0.21501916646957397, accuracy: 0.921875\n",
      "batch 140 loss: 0.24329492449760437, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.2348385602235794, accuracy: 0.9091796875\n",
      "batch 180 loss: 0.20734745264053345, accuracy: 0.9228515625\n",
      "training loss: 0.22543310165405273, accuracy: 0.916371588600056\n",
      "validation loss: 0.22461088001728058, accuracy: 0.9161389931207005\n",
      "batch 0 loss: 0.2537826895713806, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22165098786354065, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24019289016723633, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.2475818693637848, accuracy: 0.9091796875\n",
      "batch 80 loss: 0.272918164730072, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.22105880081653595, accuracy: 0.916015625\n",
      "batch 120 loss: 0.21525904536247253, accuracy: 0.921875\n",
      "batch 140 loss: 0.24311405420303345, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.23497626185417175, accuracy: 0.9091796875\n",
      "batch 180 loss: 0.20800673961639404, accuracy: 0.9208984375\n",
      "training loss: 0.2254187635332346, accuracy: 0.916357662731222\n",
      "validation loss: 0.22459769248962402, accuracy: 0.9160999061913696\n",
      "batch 0 loss: 0.25384601950645447, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22161027789115906, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24037379026412964, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.2475188970565796, accuracy: 0.9091796875\n",
      "batch 80 loss: 0.27294135093688965, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.22113965451717377, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.2150609791278839, accuracy: 0.921875\n",
      "batch 140 loss: 0.2429904043674469, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.2344742715358734, accuracy: 0.91015625\n",
      "batch 180 loss: 0.20773503184318542, accuracy: 0.9208984375\n",
      "training loss: 0.2254099867492914, accuracy: 0.916347174537556\n",
      "validation loss: 0.2246331423521042, accuracy: 0.916119449656035\n",
      "batch 0 loss: 0.2537306249141693, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22216111421585083, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24039465188980103, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24779264628887177, accuracy: 0.9091796875\n",
      "batch 80 loss: 0.27307772636413574, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.22106428444385529, accuracy: 0.916015625\n",
      "batch 120 loss: 0.21500706672668457, accuracy: 0.921875\n",
      "batch 140 loss: 0.2425863891839981, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.23479796946048737, accuracy: 0.91015625\n",
      "batch 180 loss: 0.20768627524375916, accuracy: 0.9208984375\n",
      "training loss: 0.22540445983409882, accuracy: 0.916361822975056\n",
      "validation loss: 0.2246609926223755, accuracy: 0.9160608192620388\n",
      "batch 0 loss: 0.2538605332374573, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22154748439788818, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24040749669075012, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24759036302566528, accuracy: 0.9091796875\n",
      "batch 80 loss: 0.27286607027053833, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.2209624946117401, accuracy: 0.916015625\n",
      "batch 120 loss: 0.21497952938079834, accuracy: 0.921875\n",
      "batch 140 loss: 0.24327589571475983, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.23465697467327118, accuracy: 0.91015625\n",
      "batch 180 loss: 0.20728328824043274, accuracy: 0.923828125\n",
      "training loss: 0.22537292562425137, accuracy: 0.9164064908562221\n",
      "validation loss: 0.22453951835632324, accuracy: 0.9160999061913696\n",
      "batch 0 loss: 0.2537371814250946, accuracy: 0.9140625\n",
      "batch 20 loss: 0.2215082049369812, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.2406783401966095, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.2472955882549286, accuracy: 0.9091796875\n",
      "batch 80 loss: 0.27288323640823364, accuracy: 0.8984375\n",
      "batch 100 loss: 0.2210787832736969, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21492938697338104, accuracy: 0.921875\n",
      "batch 140 loss: 0.2428421825170517, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.2349625676870346, accuracy: 0.9091796875\n",
      "batch 180 loss: 0.2072988748550415, accuracy: 0.923828125\n",
      "training loss: 0.22535039097070694, accuracy: 0.9164057682875559\n",
      "validation loss: 0.22459039092063904, accuracy: 0.916119449656035\n",
      "batch 0 loss: 0.2538461685180664, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22150269150733948, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24044737219810486, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24742574989795685, accuracy: 0.9091796875\n",
      "batch 80 loss: 0.27300330996513367, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.2210012972354889, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21493938565254211, accuracy: 0.921875\n",
      "batch 140 loss: 0.24284707009792328, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.2346530258655548, accuracy: 0.91015625\n",
      "batch 180 loss: 0.2082180380821228, accuracy: 0.9208984375\n",
      "training loss: 0.22533050194382667, accuracy: 0.916391119850056\n",
      "validation loss: 0.22461478412151337, accuracy: 0.9160412757973734\n",
      "batch 0 loss: 0.25380459427833557, accuracy: 0.9140625\n",
      "batch 20 loss: 0.2216116487979889, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24040773510932922, accuracy: 0.91015625\n",
      "batch 60 loss: 0.24719694256782532, accuracy: 0.9091796875\n",
      "batch 80 loss: 0.2728631794452667, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.22105777263641357, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.2149423062801361, accuracy: 0.921875\n",
      "batch 140 loss: 0.2427593320608139, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.23474940657615662, accuracy: 0.91015625\n",
      "batch 180 loss: 0.20722496509552002, accuracy: 0.9228515625\n",
      "training loss: 0.2253300265222788, accuracy: 0.916410651100056\n",
      "validation loss: 0.22469279170036316, accuracy: 0.9160412757973734\n",
      "batch 0 loss: 0.2540570795536041, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22169280052185059, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24043728411197662, accuracy: 0.91015625\n",
      "batch 60 loss: 0.24748925864696503, accuracy: 0.9091796875\n",
      "batch 80 loss: 0.2729876637458801, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.22100868821144104, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21499551832675934, accuracy: 0.921875\n",
      "batch 140 loss: 0.24352070689201355, accuracy: 0.900390625\n",
      "batch 160 loss: 0.2342553287744522, accuracy: 0.9111328125\n",
      "batch 180 loss: 0.20773069560527802, accuracy: 0.9208984375\n",
      "training loss: 0.22531029798090457, accuracy: 0.9163960026625559\n",
      "validation loss: 0.22458259761333466, accuracy: 0.9160217323327079\n",
      "batch 0 loss: 0.2538300156593323, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22147056460380554, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24014952778816223, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24731792509555817, accuracy: 0.9091796875\n",
      "batch 80 loss: 0.27299171686172485, accuracy: 0.8984375\n",
      "batch 100 loss: 0.2210482358932495, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21496599912643433, accuracy: 0.921875\n",
      "batch 140 loss: 0.2427000105381012, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.23444125056266785, accuracy: 0.91015625\n",
      "batch 180 loss: 0.20709887146949768, accuracy: 0.923828125\n",
      "training loss: 0.22528715141117572, accuracy: 0.916430182350056\n",
      "validation loss: 0.22453488409519196, accuracy: 0.9160999061913696\n",
      "batch 0 loss: 0.2535431385040283, accuracy: 0.9130859375\n",
      "batch 20 loss: 0.22158515453338623, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.2403029352426529, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.2472999393939972, accuracy: 0.9091796875\n",
      "batch 80 loss: 0.27297234535217285, accuracy: 0.8984375\n",
      "batch 100 loss: 0.22104349732398987, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.2149607539176941, accuracy: 0.921875\n",
      "batch 140 loss: 0.24270853400230408, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.2347138375043869, accuracy: 0.91015625\n",
      "batch 180 loss: 0.20717960596084595, accuracy: 0.9228515625\n",
      "training loss: 0.2252671480178833, accuracy: 0.916420416725056\n",
      "validation loss: 0.22455066442489624, accuracy: 0.9160803627267042\n",
      "batch 0 loss: 0.2537105977535248, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22173522412776947, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.2404124140739441, accuracy: 0.91015625\n",
      "batch 60 loss: 0.2476646453142166, accuracy: 0.9091796875\n",
      "batch 80 loss: 0.27306145429611206, accuracy: 0.8984375\n",
      "batch 100 loss: 0.22101186215877533, accuracy: 0.916015625\n",
      "batch 120 loss: 0.21495279669761658, accuracy: 0.921875\n",
      "batch 140 loss: 0.24232883751392365, accuracy: 0.90234375\n",
      "batch 160 loss: 0.23462530970573425, accuracy: 0.91015625\n",
      "batch 180 loss: 0.20772087574005127, accuracy: 0.9208984375\n",
      "training loss: 0.22526471950113774, accuracy: 0.916425299537556\n",
      "validation loss: 0.22459986805915833, accuracy: 0.9160999061913696\n",
      "batch 0 loss: 0.2538148760795593, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22149063646793365, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.24027736485004425, accuracy: 0.91015625\n",
      "batch 60 loss: 0.247574582695961, accuracy: 0.9091796875\n",
      "batch 80 loss: 0.2729487419128418, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.22109094262123108, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21501635015010834, accuracy: 0.921875\n",
      "batch 140 loss: 0.2429647296667099, accuracy: 0.900390625\n",
      "batch 160 loss: 0.23433353006839752, accuracy: 0.9111328125\n",
      "batch 180 loss: 0.20773591101169586, accuracy: 0.919921875\n",
      "training loss: 0.225234070494771, accuracy: 0.916430182350056\n",
      "validation loss: 0.22467534244060516, accuracy: 0.9160608192620388\n",
      "batch 0 loss: 0.25378966331481934, accuracy: 0.9140625\n",
      "batch 20 loss: 0.2215946614742279, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.23997589945793152, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24710693955421448, accuracy: 0.9091796875\n",
      "batch 80 loss: 0.27285856008529663, accuracy: 0.8984375\n",
      "batch 100 loss: 0.22117382287979126, accuracy: 0.9150390625\n",
      "batch 120 loss: 0.21494662761688232, accuracy: 0.921875\n",
      "batch 140 loss: 0.2425399124622345, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.23421840369701385, accuracy: 0.91015625\n",
      "batch 180 loss: 0.2076529860496521, accuracy: 0.919921875\n",
      "training loss: 0.22521486833691598, accuracy: 0.9164008854750559\n",
      "validation loss: 0.22460417449474335, accuracy: 0.9160803627267042\n",
      "batch 0 loss: 0.2536228895187378, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22161558270454407, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.2398839294910431, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.2472456693649292, accuracy: 0.9091796875\n",
      "batch 80 loss: 0.2730516195297241, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.22107470035552979, accuracy: 0.916015625\n",
      "batch 120 loss: 0.214907705783844, accuracy: 0.921875\n",
      "batch 140 loss: 0.24282963573932648, accuracy: 0.900390625\n",
      "batch 160 loss: 0.2352266013622284, accuracy: 0.9091796875\n",
      "batch 180 loss: 0.20771978795528412, accuracy: 0.9208984375\n",
      "training loss: 0.22521342433989047, accuracy: 0.916430904918722\n",
      "validation loss: 0.2245987504720688, accuracy: 0.9160803627267042\n",
      "batch 0 loss: 0.25364571809768677, accuracy: 0.9140625\n",
      "batch 20 loss: 0.22154201567173004, accuracy: 0.9208984375\n",
      "batch 40 loss: 0.23984697461128235, accuracy: 0.9091796875\n",
      "batch 60 loss: 0.24747362732887268, accuracy: 0.9091796875\n",
      "batch 80 loss: 0.27289819717407227, accuracy: 0.8974609375\n",
      "batch 100 loss: 0.22114357352256775, accuracy: 0.916015625\n",
      "batch 120 loss: 0.21491093933582306, accuracy: 0.921875\n",
      "batch 140 loss: 0.24255192279815674, accuracy: 0.9013671875\n",
      "batch 160 loss: 0.23472470045089722, accuracy: 0.9091796875\n",
      "batch 180 loss: 0.20764008164405823, accuracy: 0.9208984375\n",
      "training loss: 0.22518334835767745, accuracy: 0.916439947975056\n",
      "validation loss: 0.22469688951969147, accuracy: 0.916119449656035\n"
     ]
    }
   ],
   "source": [
    "#training loop\n",
    "epoc_hist = {'train_acc':[],'train_loss':[],'val_acc': [], 'val_loss': []}\n",
    "for epoch in range(0,epochs): #epoch loop\n",
    "    batch_acc = []\n",
    "    batch_loss = []\n",
    "    for batch in range(0,batch_count): # batch loop\n",
    "        x_batch = torch.from_numpy(x_train.iloc[batch*batch_size:(batch+1)*batch_size,:].values).to(dtype=torch.float32, device= 'cuda')\n",
    "        y_batch = torch.from_numpy(y_train.iloc[batch*batch_size:(batch+1)*batch_size].values.reshape(-1,1)).to(dtype=torch.float32, device= 'cuda')\n",
    "        probs = model.forward(x_batch)\n",
    "        loss = loss_fn(probs, y_batch)\n",
    "        optim.zero_grad() # resets the optimizer\n",
    "        loss.backward() # doing first derivative (calc gradients)\n",
    "        optim.step() # adjusting weights\n",
    "        batch_loss.append(loss.item())\n",
    "        batch_acc.append(accuracy_score(y_batch.cpu().detach().numpy(),[1 if prob > 0.5 else 0 for prob in probs.cpu().detach().numpy()]))\n",
    "        if batch%20 == 0:\n",
    "            print(f'batch {batch} loss: {batch_loss[-1]}, accuracy: {batch_acc[-1]}')\n",
    "    epoc_hist['train_acc'].append(np.average(batch_acc))  \n",
    "    epoc_hist['train_loss'].append(np.average(batch_loss))\n",
    "    probs_val = model.forward(torch.from_numpy(x_val.values).to(dtype=torch.float32, device= 'cuda'))\n",
    "    loss_val = loss_fn(probs_val, torch.from_numpy(y_val.values.reshape(-1,1)).to(dtype=torch.float32, device= 'cuda'))  \n",
    "    epoc_hist['val_acc'].append(accuracy_score(y_val,[1 if prob > 0.5 else 0 for prob in probs_val.cpu().detach().numpy()])) \n",
    "    epoc_hist['val_loss'].append(loss_val.item())\n",
    "    print(f'training loss: {epoc_hist[\"train_loss\"][-1]}, accuracy: {epoc_hist[\"train_acc\"][-1]}')\n",
    "    print(f'validation loss: {epoc_hist[\"val_loss\"][-1]}, accuracy: {epoc_hist[\"val_acc\"][-1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (10,8))\n",
    "sns.lineplot(x = range(1,epochs + 1), y = epoc_hist['train_acc'], ax = ax1)\n",
    "sns.lineplot(x = range(1,epochs + 1), y = epoc_hist['val_acc'], ax = ax1)\n",
    "sns.lineplot(x = range(1,epochs + 1), y = epoc_hist['train_loss'], ax = ax2)\n",
    "sns.lineplot(x = range(1,epochs + 1), y = epoc_hist['val_loss'], ax = ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "af5fe86b96a67f83e0fe32f7cfc6a118c7a7b5f48d34b1ed2a1c5d65e02cc133"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
